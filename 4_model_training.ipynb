{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from gee_scripts.randomforest import get_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read phu regions shapefile\n",
    "phu_regions = gpd.read_file(\"data/0_shp/AOI__Province__865_PHUs__INDONESIA.gpkg\")\n",
    "phu_regions = phu_regions.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/9_clean_training_data/clean_training_data.csv\")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\")\n",
    "# do spatial join with phu's\n",
    "df = gpd.sjoin(df, phu_regions, how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of cases per PHU\n",
    "phu_cases = df.groupby(\"phu_id\").size().reset_index(name=\"observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a boxplot of response var per region but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"island\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"PHU id\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by Island\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a boxplot of response var per region but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"source\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"source\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a boxplot of response var per region but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"province\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"PHU id\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by Province\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a boxplot showing the number of dates per each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by id and get the number of dates for each id\n",
    "df_grouped = df.groupby(\"phu_name\").count().reset_index()\n",
    "df_grouped = df_grouped[[\"phu_name\", \"date\"]]\n",
    "df_grouped.columns = [\"name\", \"date_count\"]\n",
    "df_grouped.sort_values(by=\"date_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a violin plot for the variable\n",
    "sns.violinplot(x=df_grouped[\"date_count\"])\n",
    "\n",
    "# Set the title and x-axis label\n",
    "plt.title(f\"Frequency dates per point\")\n",
    "plt.xlabel(\"Number of dates per plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all stations with less tan 9 observations\n",
    "min_obs = 9\n",
    "df.groupby('id').agg({'date': 'count'}).sort_values(by='date', ascending=False).reset_index()\n",
    "df = df.groupby('id').filter(lambda group: len(group) >= min_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.parameters import explain_vars, response_var\n",
    "print(\"dependent var\", response_var)\n",
    "print(\"explanatory lenght\", len(explain_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All but one test over stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.randomforest import run_randomforest\n",
    "from gee_scripts.randomforest import get_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'gwl_cm'\n",
    "\n",
    "\n",
    "high_corr_ids = pd.read_csv(\"data/high_corr_0.2_temporal_variables_station_ids.csv\")\n",
    "high_corr_ids.columns = [\"id\"]\n",
    "\n",
    "training_df = df[\n",
    "    (df.island == \"Kalimantan\") &\n",
    "    (df.id.isin(high_corr_ids.id.unique()))\n",
    "]\n",
    "\n",
    "# Manually selected PHU for training\n",
    "# high_corr_phu_ids = [\n",
    "#     136,\n",
    "#     137,\n",
    "#     138,\n",
    "#     143\n",
    "# ]\n",
    "# training_df = df[\n",
    "#     (df.phu_id.isin(high_corr_phu_ids))\n",
    "# ]\n",
    "\n",
    "stats_df = run_randomforest(training_df, type_=\"allbutone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"r_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"rmse_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change parameters, play with different datasets\n",
    "\n",
    "training_df = df[\n",
    "    (df.island == \"Kalimantan\") & \n",
    "    (df.id.isin(high_corr_ids.id.unique()))\n",
    "]\n",
    "stats_df = run_randomforest(training_df, type_=\"allbutone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of values per station\n",
    "training_df.groupby('id').agg({'date': 'count'}).sort_values(by='date', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stations = stats_df[stats_df.r_local > 0.5].sort_values(by=\"r_local\", ascending=False).index\n",
    "best_stations\n",
    "len(best_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with best stations over all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from gee_scripts.parameters import explain_vars, temporal_expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split gdf into good statoins for train\n",
    "gdf_high = training_df[training_df.id.isin(best_stations)].copy()\n",
    "# and bad stations for test\n",
    "gdf_low = training_df[~training_df.id.isin(best_stations)].copy()\n",
    "\n",
    "variable = 'gwl_cm'\n",
    "\n",
    "# create and train classifier\n",
    "regr = get_regressor()\n",
    "\n",
    "regr.fit(gdf_high[explain_vars], gdf_high[variable])\n",
    "\n",
    "\n",
    "row = {}\n",
    "#rmse_list = []\n",
    "for station in gdf_low.id.unique():\n",
    "    explans = []\n",
    "    # apply model to specific station\n",
    "    gdf_test = gdf_low[gdf_low.id == station]\n",
    "    y_pred_test = regr.predict(gdf_test[explain_vars])\n",
    "\n",
    "    # get pearsons r\n",
    "    r, p = pearsonr(gdf_test[variable].values, y_pred_test)\n",
    "    explans.append(r)\n",
    "\n",
    "    explans.append(np.sqrt(mean_squared_error(gdf_test[variable].values, y_pred_test)))\n",
    "\n",
    "    # add correlation of explanatories\n",
    "    for expl in temporal_expl:\n",
    "        explans.append(gdf_test[variable].corr(gdf_test[expl]))\n",
    "     \n",
    "    row[station] = explans\n",
    "    #row[station] = [np.sqrt(mean_squared_error(gdf_test[variable].values, y_pred_test))]\n",
    "    #print(row)\n",
    "    \n",
    "stats_df = pd.DataFrame.from_dict(row, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"r_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"rmse_local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_high.to_file(\"data/0_shp/kalimantan_best_stations.gpkg\", driver=\"GPKG\")\n",
    "len(gdf_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worse_stations = stats_df[stats_df.rmse_local < 10].index\n",
    "best_worse_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model bootstraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_stations = list(best_stations) #+ list(best_worse_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.8\n",
    "size = int(train_size*len(bootstrap_stations))\n",
    "print(size)\n",
    "\n",
    "i = 0\n",
    "r_list, r2_list, rmse_list = [], [], []\n",
    "samples_train, samples_test = [], []\n",
    "\n",
    "i = 0\n",
    "while i < 100: \n",
    "\n",
    "    train_list = np.random.choice(bootstrap_stations, size=size, replace=False)\n",
    "\n",
    "\n",
    "    gdf_train = training_df[training_df.id.isin(train_list)].copy()\n",
    "\n",
    "    gdf_test = training_df[\n",
    "        (training_df.id.isin(bootstrap_stations)) & (~training_df.id.isin(gdf_train.id.unique()))\n",
    "    ].copy()\n",
    "\n",
    "    X_train, X_test = gdf_train[explain_vars], gdf_test[explain_vars]\n",
    "    y_train, y_test = gdf_train[variable], gdf_test[variable]\n",
    "    \n",
    "    regr = get_regressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred_test = regr.predict(X_test)\n",
    "    \n",
    "    samples_train.append(len(gdf_train))\n",
    "    samples_test.append(len(gdf_test))\n",
    "    r, p = pearsonr(y_test, y_pred_test)\n",
    "    r_list.append(r)\n",
    "    r2_list.append(r2_score(y_test, y_pred_test))\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    [np.array(r_list).mean(), np.array(r_list).min(), np.array(r_list).max(), np.median(np.array(r_list))],\n",
    "    [np.array(r2_list).mean(), np.array(r2_list).min(), np.array(r2_list).max(), np.median(np.array(r2_list))],\n",
    "    [np.array(rmse_list).mean(), np.array(rmse_list).min(), np.array(rmse_list).max(), np.median(np.array(rmse_list))],\n",
    "    [np.array(samples_train).mean(), np.array(samples_train).min(), np.array(samples_train).max()],\n",
    "    [np.array(samples_test).mean(), np.array(samples_test).min(), np.array(samples_test).max()],\n",
    "],\n",
    "    index=[\"r\", \"r2\", \"rmse\", \"samples_train\", \"samples_test\"],\n",
    "    columns=[\"mean\", \"min\", \"max\", \"median\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to file with not pickle (pickle is not safe) \n",
    "import joblib\n",
    "model_name = \"All_but_one_PHU_Kalimantan_high_corr_0_2_temporal_variables\"\n",
    "joblib.dump(regr, f\"data/10_models/{model_name}.joblib\")\n",
    "plt.savefig(f\"{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open unieqe points geopackage file\n",
    "unique_points = gpd.read_file(\"data/0_shp/unique_stations_no_repeated.shp\")\n",
    "\n",
    "# export best estations to geopackage\n",
    "unique_points[unique_points.id.isin(best_stations)].to_file(\"data/0_shp/best_kalimantan_corr02_rmse10.gpkg\", driver='GPKG')\n",
    "\n",
    "# # merge the stats_df with the unique_points\n",
    "# unique_points = unique_points.merge(stats_df, left_on='id', right_index=True)\n",
    "\n",
    "# # save the unique_points as a geopackage file\n",
    "# unique_points.to_file(\"data/0_shp/kalimantan_r_local.gpkg\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.sort_values(by=\"r_local\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(test) gwl-modeling",
   "language": "python",
   "name": "gwl-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
