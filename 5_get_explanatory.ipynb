{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "# related. \n",
    "# In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "# EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "# calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>date</th>\n",
       "      <th>gwl_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ach</td>\n",
       "      <td>BRG_140301_01</td>\n",
       "      <td>102.099167</td>\n",
       "      <td>1.519444</td>\n",
       "      <td>2018-10-15</td>\n",
       "      <td>-14.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ach</td>\n",
       "      <td>BRG_140301_01</td>\n",
       "      <td>102.099167</td>\n",
       "      <td>1.519444</td>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>-17.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ach</td>\n",
       "      <td>BRG_140301_01</td>\n",
       "      <td>102.099167</td>\n",
       "      <td>1.519444</td>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>-20.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ach</td>\n",
       "      <td>BRG_140301_01</td>\n",
       "      <td>102.099167</td>\n",
       "      <td>1.519444</td>\n",
       "      <td>2018-10-18</td>\n",
       "      <td>-18.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ach</td>\n",
       "      <td>BRG_140301_01</td>\n",
       "      <td>102.099167</td>\n",
       "      <td>1.519444</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>-23.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274261</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>-3.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274262</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>-3.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274263</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>-3.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274264</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>2019-11-02</td>\n",
       "      <td>-3.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274265</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>-3.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274266 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source             id         lon       lat       date  gwl_cm\n",
       "0           ach  BRG_140301_01  102.099167  1.519444 2018-10-15 -14.400\n",
       "1           ach  BRG_140301_01  102.099167  1.519444 2018-10-16 -17.900\n",
       "2           ach  BRG_140301_01  102.099167  1.519444 2018-10-17 -20.600\n",
       "3           ach  BRG_140301_01  102.099167  1.519444 2018-10-18 -18.100\n",
       "4           ach  BRG_140301_01  102.099167  1.519444 2018-10-19 -23.100\n",
       "...         ...            ...         ...       ...        ...     ...\n",
       "274261  old_brg         kecil1  113.805611 -2.856089 2019-10-26  -3.021\n",
       "274262  old_brg         kecil1  113.805611 -2.856089 2019-10-27  -3.023\n",
       "274263  old_brg         kecil1  113.805611 -2.856089 2019-10-31  -3.023\n",
       "274264  old_brg         kecil1  113.805611 -2.856089 2019-11-02  -3.023\n",
       "274265  old_brg         kecil1  113.805611 -2.856089 2019-11-04  -3.022\n",
       "\n",
       "[274266 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords.csv')\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "high_corr_ids = pd.read_csv('data/ids_high_corr.csv')\n",
    "len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[df.id.isin(high_corr_ids.id)][[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)\n",
    "\n",
    "# Convert them as a geodataframe and save them\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)\n",
    "# gdf.crs = {'init': 'epsg:4326'}\n",
    "# gdf.to_file(\"data/0_shp/high_corr_stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((116.84967 3.98347, 117.30926 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           geometry\n",
       "0   1  MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...\n",
       "1   2  MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...\n",
       "2   3  MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...\n",
       "3   4  MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...\n",
       "4   5  MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...\n",
       "5   6  MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...\n",
       "6   7  MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...\n",
       "7   8  MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...\n",
       "8   9  MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...\n",
       "9  10  MULTIPOLYGON (((116.84967 3.98347, 117.30926 3..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "\n",
    "# I have two regions, first is to only the high correlated stations and the second is all the stations\n",
    "# gdf_regions = gpd.GeoDataFrame.from_file(\"data/0_shp/regions_to_request_explanatory.gpkg\")\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(\"data/0_shp/regions_to_request_explanatory_all.gpkg\")\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260942, 2074)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords.csv')\n",
    "df[\"date\"] = pd.to_datetime(df.date)\n",
    "\n",
    "# To only get the high correlated stations, uncomment the following lines\n",
    "# high_corr_ids = pd.read_csv('data/ids_high_corr.csv')\n",
    "# df = df[df['id'].isin(high_corr_ids['id'])]\n",
    "\n",
    "# Remove those date where the gwl measure is out of reasonable range\n",
    "upper_thres = 20\n",
    "lower_thres = -100\n",
    "\n",
    "df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# save the final points\n",
    "\n",
    "# df.to_csv('field_data_high_corr.csv', index=False)\n",
    "\n",
    "# Get the coordinates of the individual points\n",
    "\n",
    "unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_image, get_gldas, get_gpm\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>id_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93142</th>\n",
       "      <td>15_RAPP_TP-I-53</td>\n",
       "      <td>102.015869</td>\n",
       "      <td>0.563137</td>\n",
       "      <td>POINT (102.01587 0.56314)</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id_left         lon       lat                   geometry  \\\n",
       "93142  15_RAPP_TP-I-53  102.015869  0.563137  POINT (102.01587 0.56314)   \n",
       "\n",
       "       index_right  id_right  \n",
       "93142            4         5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "# Add the region id to each point\n",
    "\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"id\", \"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "\n",
    "\n",
    "# I need to extract all the dates from the first group of points\n",
    "# first get the ids of the first group of points\n",
    "\n",
    "gdf_unique_coords[gdf_unique_coords.id_left == \"15_RAPP_TP-I-53\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get temporal explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date\n",
      "All_temporal_non_resample_at_all_region_2_dates_1773_points_149_with_date\n",
      "All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date\n",
      "All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date\n",
      "All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date\n",
      "All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date\n",
      "All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date\n",
      "All_temporal_non_resample_at_all_region_8_dates_1671_points_221_with_date\n",
      "All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date\n",
      "All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_selectors = [\"system:index\", \"lat\", \"lon\", \"id\", \"date\"]\n",
    "s1_selectors = [\"LIA\", \"VH\", \"VV\", \"VVVH_ratio\", \"angle\"]\n",
    "gldas_selectors = ['sm_1', 'sm_3', 'sm_7', 'sm_30']\n",
    "gpm_selectors = ['precipitation', 'prec_3', 'prec_7', 'prec_30']\n",
    "\n",
    "\n",
    "def get_temporal_explanatory(region_id):\n",
    "   \"\"\"Get the explanatory temporal based variables\"\"\"\n",
    "\n",
    "   region = gdf_regions[gdf_regions.id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "   dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.id_right == region_id].id_left.unique())].date.unique()\n",
    "   points = gdf_unique_coords[gdf_unique_coords.id_right == region_id][[\"id_left\", \"geometry\"]].rename(columns={\"id_left\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "\n",
    "   # print(len(dates), len(points))\n",
    "   # Convert to ee elements\n",
    "\n",
    "   ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "   ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "   ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "   def get_sources(date_feature):\n",
    "      \n",
    "      date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "      s1_composite = get_s1_image(date_range, ee_region)\n",
    "\n",
    "      return s1_composite.set({\n",
    "         \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "         \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "         })\n",
    "      \n",
    "   def reduce_composite(composite):\n",
    "      \n",
    "      # Filter the extra data with the matching date\n",
    "      date = composite.get(\"date\")\n",
    "      date_range = ee.Date(date).getRange('day')\n",
    "\n",
    "      gldas_composite = get_gldas(date_range, ee_region)\n",
    "      gpm_composite = get_gpm(date_range, ee_region)\n",
    "\n",
    "      composite = (ee.Image(composite)\n",
    "            .addBands(gldas_composite)\n",
    "            .addBands(gpm_composite)\n",
    "      )\n",
    "      \n",
    "      return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "      }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "         \"date\" : date\n",
    "      }))\n",
    "\n",
    "\n",
    "   task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "   )\n",
    "\n",
    "   task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date\"\n",
    "\n",
    "   ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "      \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors\n",
    "   })\n",
    "\n",
    "   # Uncoment to start the task\n",
    "   ee_task.start()\n",
    "   \n",
    "   print(task_name)\n",
    "\n",
    "[get_temporal_explanatory(region_id) for region_id in gdf_regions.id.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.get_sources import get_hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_selectors = [\"year\", \"B3\",\"B4\",\"B5\",\"B7\",\"ndvi\",\"ndmi\",\"ndbri\"]\n",
    "\n",
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013] )\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "    points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "    image = get_hansen(year)\n",
    "\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": f\"Hansen_year_{year}_points_{len(points)}_f\",\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "\n",
    "    print(f\"Hansen_year_{year}_points_{len(points)}_f\")\n",
    "\n",
    "    # ee_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id_left\", \"geometry\"]].rename(columns={\"id_left\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    ")\n",
    "composite.bandNames().getInfo()\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\": f\"All_Non_temporal_points_{len(points)}\",\n",
    "    \"selectors\": base_selectors + ['elevation', 'aspect', 'slope', 'land_cov', 'canopy_height', \"gldas_mean\", \"gldas_stddev\"]\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "# ee_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "temporal_file_names = [\n",
    "    \"High_corr_All_temporal_non_resample_region_1_dates_485_points_2.csv\",\n",
    "    \"High_corr_All_temporal_non_resample_region_2_dates_626_points_11.csv\",\n",
    "    \"High_corr_All_temporal_non_resample_region_3_dates_1737_points_13.csv\",\n",
    "    \"High_corr_All_temporal_non_resample_region_4_dates_653_points_12.csv\",\n",
    "    \"High_corr_All_temporal_non_resample_region_5_dates_1542_points_21.csv\",\n",
    "    \"High_corr_All_temporal_non_resample_region_6_dates_479_points_1.csv\",\n",
    "]\n",
    "\n",
    "# temporal_file_names = [\n",
    "#     \"All_temporal_non_resample_region_10_dates_846_points_77.csv\",\n",
    "#     \"All_temporal_non_resample_region_1_dates_520_points_24.csv\",\n",
    "#     \"All_temporal_non_resample_region_2_dates_1773_points_149.csv\",\n",
    "#     \"All_temporal_non_resample_region_3_dates_479_points_1.csv\",\n",
    "#     \"All_temporal_non_resample_region_4_dates_988_points_348.csv\",\n",
    "#     \"All_temporal_non_resample_region_5_dates_1796_points_717.csv\",\n",
    "#     \"All_temporal_non_resample_region_6_dates_489_points_43.csv\",\n",
    "#     \"All_temporal_non_resample_region_7_dates_1274_points_477.csv\",\n",
    "#     \"All_temporal_non_resample_region_8_dates_1671_points_221.csv\",\n",
    "#     \"All_temporal_non_resample_region_9_dates_379_points_17.csv\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_cols = [\n",
    "    'id', 'lat', 'lon', \"date\", 'LIA', 'VH', 'VV', 'VVVH_ratio', 'angle',\n",
    "    'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation',\n",
    "    'prec_3', 'prec_7', 'prec_30'\n",
    "]\n",
    "\n",
    "def add_date_to_explanatory_df(region_id, explain_df):\n",
    "    \"\"\"Add the corresponding date to the explanatory dataframe.\n",
    "\n",
    "    As the result from GEE didn't come with the date, we'll need to add it manually.\n",
    "    For each of the .csv results, we have to use the \"dates\" list that was used to get the data,\n",
    "    and by its index, we can merge the date to the dataframe.\n",
    "    \n",
    "    For each region we will have different dates.\n",
    "    \"\"\"\n",
    "\n",
    "    dates = pd.DataFrame(\n",
    "        df[df.id.isin(gdf_unique_coords[gdf_unique_coords.id_right == region_id].id_left.unique())].date.unique(),\n",
    "        columns=[\"date\"]\n",
    "    ).reset_index()\n",
    "\n",
    "    # Get the date of the measurement based on the \"system:index\" col\n",
    "    explain_df[\"date_idx\"] = explain_df[\"system:index\"].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "\n",
    "    # return explain_df.merge(dates, left_on=\"date_idx\", right_on=\"index\")\n",
    "    return explain_df.merge(dates, left_on=\"date_idx\", right_on=\"index\")[temporal_cols]\n",
    "\n",
    "\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "    add_date_to_explanatory_df(region_id, explain_df) \n",
    "    for region_id, explain_df \n",
    "    in enumerate(\n",
    "        [\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    "        start=1\n",
    "        )\n",
    "    ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have checked this value and compared it with the latest request I did when I assigned directly the date\n",
    "assert temp_explanatory_dfs[(temp_explanatory_dfs.id==\"15_RAPP_TP-I-53\") & (temp_explanatory_dfs.date == \"2022-11-22\")][\"LIA\"].values[0] == 33.80583090268845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that dates assignation are correct\n",
    "assert temp_explanatory_dfs[temp_explanatory_dfs.id == \"BRG_910111_01\"].date.dt.year.unique().tolist() == [2018, 2019, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"Hansen_year_2023_points_2075_f.csv\"\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(explanatory_path/file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_file_name = \"All_Non_temporal_points_2074.csv\"\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)\n",
    "\n",
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vars = [\n",
    "    'id', 'date', 'LIA', 'VH', 'VV', 'VVVH_ratio',\n",
    "    'angle', 'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation', 'prec_3',\n",
    "    'prec_7', 'prec_30', 'elevation',\n",
    "    'aspect', 'slope', 'land_cov', 'canopy_height', 'gldas_mean',\n",
    "    'gldas_stddev', 'B3', 'B4',\n",
    "    'B5', 'B7', 'ndvi', 'ndmi', 'ndbri'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL STEP: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var = df[[\"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(explanatory_df[export_vars], on=[\"id\", \"date\"])\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "explanatory_with_response_var.to_csv(\"data/7_training_data/explanatory_with_response_var.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
