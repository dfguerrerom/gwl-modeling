{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "related. \n",
    "In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_image, get_gldas, get_gpm, get_hansen, get_gpm_sum\n",
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats, get_extra_non_temporal\n",
    "from gee_scripts import init_ee\n",
    "init_ee()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>gwl_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2020-11-05</td>\n",
       "      <td>pkeg</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>-37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>pkeg</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>pkeg</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>pkeg</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>-35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>pkeg</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267213</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>old_brg</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>-302.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267214</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>old_brg</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>-302.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267215</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>old_brg</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>-302.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267216</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>2019-11-02</td>\n",
       "      <td>old_brg</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>-302.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267217</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>old_brg</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>-302.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267218 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       date   source         lon       lat  gwl_cm\n",
       "0       02_AHL_SBG-B076 2020-11-05     pkeg  117.007750  3.937760   -37.0\n",
       "1       02_AHL_SBG-B076 2020-11-17     pkeg  117.007750  3.937760   -39.0\n",
       "2       02_AHL_SBG-B076 2020-12-05     pkeg  117.007750  3.937760   -39.0\n",
       "3       02_AHL_SBG-B076 2020-12-16     pkeg  117.007750  3.937760   -35.0\n",
       "4       02_AHL_SBG-B076 2021-01-02     pkeg  117.007750  3.937760   -34.0\n",
       "...                 ...        ...      ...         ...       ...     ...\n",
       "267213           kecil1 2019-10-26  old_brg  113.805611 -2.856089  -302.1\n",
       "267214           kecil1 2019-10-27  old_brg  113.805611 -2.856089  -302.3\n",
       "267215           kecil1 2019-10-31  old_brg  113.805611 -2.856089  -302.3\n",
       "267216           kecil1 2019-11-02  old_brg  113.805611 -2.856089  -302.3\n",
       "267217           kecil1 2019-11-04  old_brg  113.805611 -2.856089  -302.2\n",
       "\n",
       "[267218 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords.csv')\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "## Set type of output\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run entirely, if we want to task the orders to GEE we'll set this variable to True\n",
    "send_task = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)\n",
    "\n",
    "# Convert them as a geodataframe and save them\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((116.84967 3.98347, 117.30926 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_id                                           geometry\n",
       "0          1  MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...\n",
       "1          2  MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...\n",
       "2          3  MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...\n",
       "3          4  MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...\n",
       "4          5  MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...\n",
       "5          6  MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...\n",
       "6          7  MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...\n",
       "7          8  MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...\n",
       "8          9  MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...\n",
       "9         10  MULTIPOLYGON (((116.84967 3.98347, 117.30926 3..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "# I have two regions, first is to only the high correlated stations and the second is all the stations\n",
    "# I will use either depending on the dataset we have selected above\n",
    "\n",
    "shp_path = Path(\"data/0_shp/\")\n",
    "region_path = \"regions_to_request_explanatory_all.gpkg\"\n",
    "\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(shp_path/region_path)\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247620, 2072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove those date where the gwl measure is out of reasonable range\n",
    "upper_thres = 20\n",
    "lower_thres = -100\n",
    "\n",
    "df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# Get the coordinates of the individual points\n",
    "\n",
    "unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_coors = ['138_NBR_M13','18_SBA_DTD043','18_SBA_DTP002','18_SBA_DTP025','18_SBA_DTP031','18_SBA_DTP034','18_SBA_DTP038','18_SBA_DTP054','271_RSP_H19','BRG_140301_01','BRG_140302_01','BRG_140302_02','BRG_610117_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Add the region id to each point\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"region_id\", \"geometry\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222645</th>\n",
       "      <td>BRG_150710_01</td>\n",
       "      <td>103.900168</td>\n",
       "      <td>-1.274317</td>\n",
       "      <td>POINT (103.90017 -1.27432)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245948</th>\n",
       "      <td>BRG_621107_03</td>\n",
       "      <td>114.220600</td>\n",
       "      <td>-2.654022</td>\n",
       "      <td>POINT (114.22060 -2.65402)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id         lon       lat                    geometry  \\\n",
       "222645  BRG_150710_01  103.900168 -1.274317  POINT (103.90017 -1.27432)   \n",
       "245948  BRG_621107_03  114.220600 -2.654022  POINT (114.22060 -2.65402)   \n",
       "\n",
       "        index_right  region_id  \n",
       "222645            1          2  \n",
       "245948            7          8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Failing datasets array(['BRG_150710_01', 'BRG_621107_03'], dtype=object)\n",
    "gdf_unique_coords[gdf_unique_coords.id.isin([\"BRG_621107_03\", \"BRG_150710_01\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>POINT (117.00775 3.93776)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>02_AHL_SBG-B101</td>\n",
       "      <td>117.010120</td>\n",
       "      <td>3.931860</td>\n",
       "      <td>POINT (117.01012 3.93186)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>02_AHL_SBG-B103</td>\n",
       "      <td>117.005210</td>\n",
       "      <td>3.926090</td>\n",
       "      <td>POINT (117.00521 3.92609)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>02_AHL_SBG-C003</td>\n",
       "      <td>117.145430</td>\n",
       "      <td>3.903400</td>\n",
       "      <td>POINT (117.14543 3.90340)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>02_AHL_SBG-C006</td>\n",
       "      <td>117.148320</td>\n",
       "      <td>3.919380</td>\n",
       "      <td>POINT (117.14832 3.91938)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264584</th>\n",
       "      <td>brg6</td>\n",
       "      <td>103.969297</td>\n",
       "      <td>-1.442811</td>\n",
       "      <td>POINT (103.96930 -1.44281)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264984</th>\n",
       "      <td>brg8</td>\n",
       "      <td>102.390639</td>\n",
       "      <td>0.738492</td>\n",
       "      <td>POINT (102.39064 0.73849)</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266206</th>\n",
       "      <td>jambi1</td>\n",
       "      <td>103.589975</td>\n",
       "      <td>-1.238478</td>\n",
       "      <td>POINT (103.58997 -1.23848)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266449</th>\n",
       "      <td>kalbar1</td>\n",
       "      <td>109.394853</td>\n",
       "      <td>-0.210225</td>\n",
       "      <td>POINT (109.39485 -0.21022)</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266842</th>\n",
       "      <td>kalteng1</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>POINT (114.05813 -2.31973)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2072 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id         lon       lat                    geometry  \\\n",
       "0       02_AHL_SBG-B076  117.007750  3.937760   POINT (117.00775 3.93776)   \n",
       "65      02_AHL_SBG-B101  117.010120  3.931860   POINT (117.01012 3.93186)   \n",
       "130     02_AHL_SBG-B103  117.005210  3.926090   POINT (117.00521 3.92609)   \n",
       "195     02_AHL_SBG-C003  117.145430  3.903400   POINT (117.14543 3.90340)   \n",
       "260     02_AHL_SBG-C006  117.148320  3.919380   POINT (117.14832 3.91938)   \n",
       "...                 ...         ...       ...                         ...   \n",
       "264584             brg6  103.969297 -1.442811  POINT (103.96930 -1.44281)   \n",
       "264984             brg8  102.390639  0.738492   POINT (102.39064 0.73849)   \n",
       "266206           jambi1  103.589975 -1.238478  POINT (103.58997 -1.23848)   \n",
       "266449          kalbar1  109.394853 -0.210225  POINT (109.39485 -0.21022)   \n",
       "266842         kalteng1  114.058131 -2.319728  POINT (114.05813 -2.31973)   \n",
       "\n",
       "        index_right  region_id  \n",
       "0                 9         10  \n",
       "65                9         10  \n",
       "130               9         10  \n",
       "195               9         10  \n",
       "260               9         10  \n",
       "...             ...        ...  \n",
       "264584            1          2  \n",
       "264984            4          5  \n",
       "266206            1          2  \n",
       "266449            6          7  \n",
       "266842            7          8  \n",
       "\n",
       "[2072 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get temporal explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((116.84967 3.98347, 117.30926 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_id                                           geometry\n",
       "0          1  MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...\n",
       "1          2  MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...\n",
       "2          3  MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...\n",
       "3          4  MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...\n",
       "4          5  MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...\n",
       "5          6  MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...\n",
       "6          7  MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...\n",
       "7          8  MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...\n",
       "8          9  MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...\n",
       "9         10  MULTIPOLYGON (((116.84967 3.98347, 117.30926 3..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>POINT (117.00775 3.93776)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>02_AHL_SBG-B101</td>\n",
       "      <td>117.010120</td>\n",
       "      <td>3.931860</td>\n",
       "      <td>POINT (117.01012 3.93186)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>02_AHL_SBG-B103</td>\n",
       "      <td>117.005210</td>\n",
       "      <td>3.926090</td>\n",
       "      <td>POINT (117.00521 3.92609)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>02_AHL_SBG-C003</td>\n",
       "      <td>117.145430</td>\n",
       "      <td>3.903400</td>\n",
       "      <td>POINT (117.14543 3.90340)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>02_AHL_SBG-C006</td>\n",
       "      <td>117.148320</td>\n",
       "      <td>3.919380</td>\n",
       "      <td>POINT (117.14832 3.91938)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264584</th>\n",
       "      <td>brg6</td>\n",
       "      <td>103.969297</td>\n",
       "      <td>-1.442811</td>\n",
       "      <td>POINT (103.96930 -1.44281)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264984</th>\n",
       "      <td>brg8</td>\n",
       "      <td>102.390639</td>\n",
       "      <td>0.738492</td>\n",
       "      <td>POINT (102.39064 0.73849)</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266206</th>\n",
       "      <td>jambi1</td>\n",
       "      <td>103.589975</td>\n",
       "      <td>-1.238478</td>\n",
       "      <td>POINT (103.58997 -1.23848)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266449</th>\n",
       "      <td>kalbar1</td>\n",
       "      <td>109.394853</td>\n",
       "      <td>-0.210225</td>\n",
       "      <td>POINT (109.39485 -0.21022)</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266842</th>\n",
       "      <td>kalteng1</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>POINT (114.05813 -2.31973)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2072 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id         lon       lat                    geometry  \\\n",
       "0       02_AHL_SBG-B076  117.007750  3.937760   POINT (117.00775 3.93776)   \n",
       "65      02_AHL_SBG-B101  117.010120  3.931860   POINT (117.01012 3.93186)   \n",
       "130     02_AHL_SBG-B103  117.005210  3.926090   POINT (117.00521 3.92609)   \n",
       "195     02_AHL_SBG-C003  117.145430  3.903400   POINT (117.14543 3.90340)   \n",
       "260     02_AHL_SBG-C006  117.148320  3.919380   POINT (117.14832 3.91938)   \n",
       "...                 ...         ...       ...                         ...   \n",
       "264584             brg6  103.969297 -1.442811  POINT (103.96930 -1.44281)   \n",
       "264984             brg8  102.390639  0.738492   POINT (102.39064 0.73849)   \n",
       "266206           jambi1  103.589975 -1.238478  POINT (103.58997 -1.23848)   \n",
       "266449          kalbar1  109.394853 -0.210225  POINT (109.39485 -0.21022)   \n",
       "266842         kalteng1  114.058131 -2.319728  POINT (114.05813 -2.31973)   \n",
       "\n",
       "        index_right  region_id  \n",
       "0                 9         10  \n",
       "65                9         10  \n",
       "130               9         10  \n",
       "195               9         10  \n",
       "260               9         10  \n",
       "...             ...        ...  \n",
       "264584            1          2  \n",
       "264984            4          5  \n",
       "266206            1          2  \n",
       "266449            6          7  \n",
       "266842            7          8  \n",
       "\n",
       "[2072 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 0_Precipitation_sum_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_3_dates_362_points_1_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_5_dates_1796_points_718_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_7_dates_1273_points_477_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_8_dates_1671_points_219_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat\n",
      "Exported 0_Precipitation_sum_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_selectors = [\"system:index\", \"lat\", \"lon\", \"id\", \"date\"]\n",
    "s1_selectors = [\"LIA\", \"VH\", \"VV\", \"VVVH_ratio\", \"angle\"]\n",
    "gldas_selectors = ['sm_1', 'sm_3', 'sm_7', 'sm_30']\n",
    "gpm_selectors = ['precipitation', 'prec_3', 'prec_7', 'prec_30']\n",
    "gpm_selectors_sum = ['prec_3_sum', 'prec_7_sum', 'prec_30_sum']\n",
    "\n",
    "def get_temporal_explanatory(region_id):\n",
    "    \"\"\"Get the explanatory temporal based variables\"\"\"\n",
    "\n",
    "    region = gdf_regions[gdf_regions.region_id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "    dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.region_id == region_id].id.unique())].date.unique()\n",
    "    points = gdf_unique_coords[gdf_unique_coords.region_id == region_id][[\"id\", \"geometry\", \"lat\", \"lon\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # print(len(dates), len(points))\n",
    "    # Convert to ee elements\n",
    "\n",
    "    ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "    ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "    def get_sources(date_feature):\n",
    "\n",
    "        date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "        s1_composite = get_s1_image(date_range, ee_region)\n",
    "\n",
    "        return s1_composite.set({\n",
    "         \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "         \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "         })\n",
    "\n",
    "    def reduce_composite(composite):\n",
    "\n",
    "        # Filter the extra data with the matching date\n",
    "        date = composite.get(\"date\")\n",
    "        date_range = ee.Date(date).getRange('day')\n",
    "\n",
    "        gldas_composite = get_gldas(date_range, ee_region)\n",
    "        gpm_composite = get_gpm(date_range, ee_region)\n",
    "        gpm_sum_composite = get_gpm_sum(date_range, ee_region)\n",
    "\n",
    "        composite = (ee.Image(composite)\n",
    "            .addBands(gpm_sum_composite)\n",
    "        )\n",
    "\n",
    "        # composite = (ee.Image(composite)\n",
    "        #     .addBands(gldas_composite)\n",
    "        #     .addBands(gpm_composite)\n",
    "        # )\n",
    "\n",
    "        return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "        }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "         \"date\" : date\n",
    "        }))\n",
    "\n",
    "\n",
    "    task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "    )\n",
    "\n",
    "    # task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "    task_name = f\"0_Precipitation_sum_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "    #   \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors\n",
    "      \"selectors\": base_selectors + gpm_selectors_sum\n",
    "\n",
    "    })\n",
    "\n",
    "    # Uncoment to start the task\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "send_task = True\n",
    "[get_temporal_explanatory(region_id) for region_id in gdf_regions.region_id.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_selectors = [\"year\", \"B3\",\"B4\",\"B5\",\"B7\",\"ndvi\",\"ndmi\",\"ndbri\"]\n",
    "\n",
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013] )\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "    points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "    image = get_hansen(year)\n",
    "\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "    \n",
    "    task_name = f\"Hansen_year_{year}_points_{len(points)}_f\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": f\"Hansen_year_{year}_points_{len(points)}_f\",\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is not too computational expensive, so we are not forced to chunk it\n",
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id\", \"geometry\", \"lat\", \"lon\"]].rename(columns={\"id\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    ")\n",
    "composite.bandNames().getInfo()\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "task_name = f\"All_Non_temporal_points_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['elevation', 'aspect', 'slope', 'land_cov', 'canopy_height', \"gldas_mean\", \"gldas_stddev\"]\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Extra Non temporal explanatory variables (others)\n",
    "\n",
    "This data comes from https://code.earthengine.google.com/6c3eeb929a5ee8a42f55234b58796c0a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = get_extra_non_temporal()\n",
    "composite.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phu = ee.FeatureCollection(\n",
    "    \"users/marortpab/FAO/SEPAL/2023_trainings/smm/AOI__Province__865_PHUs__INDONESIA\"\n",
    ")\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['distance']))\n",
    "\n",
    "task_name = f\"All_Non_temporal_extra_points_latlon_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['distance', 'dir', 'acc']\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "dataset = \"all\"\n",
    "temporal_file_names_groups = {\n",
    "    \"all\" : [\n",
    "        \"All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_8_dates_1671_points_220_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I modified the notebook on the 31/05/2023 to include the sum of the precipitation\n",
    "temporal_precip_sum = {\n",
    "    \"all\" : [\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_3_dates_362_points_1_with_date_lon_lat.csv\", # Using this there's only 362 where the other has 479\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_5_dates_1796_points_718_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_7_dates_1273_points_477_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_8_dates_1671_points_219_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "# temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system:index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>prec_3_sum</th>\n",
       "      <th>prec_7_sum</th>\n",
       "      <th>prec_30_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13_40018</td>\n",
       "      <td>3.809687</td>\n",
       "      <td>96.451939</td>\n",
       "      <td>121_APC_A21</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>7.72</td>\n",
       "      <td>8.98</td>\n",
       "      <td>137.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13_40056</td>\n",
       "      <td>3.811906</td>\n",
       "      <td>96.468786</td>\n",
       "      <td>121_APC_B09</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>7.72</td>\n",
       "      <td>8.98</td>\n",
       "      <td>137.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_40094</td>\n",
       "      <td>3.806685</td>\n",
       "      <td>96.459393</td>\n",
       "      <td>121_APC_B14</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>7.72</td>\n",
       "      <td>8.98</td>\n",
       "      <td>137.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_40132</td>\n",
       "      <td>3.855798</td>\n",
       "      <td>96.523820</td>\n",
       "      <td>121_APC_D11</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.17</td>\n",
       "      <td>120.049997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13_40170</td>\n",
       "      <td>3.850565</td>\n",
       "      <td>96.515498</td>\n",
       "      <td>121_APC_D15</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.17</td>\n",
       "      <td>120.049997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>840_5023</td>\n",
       "      <td>3.562450</td>\n",
       "      <td>117.137710</td>\n",
       "      <td>02_AHL_SSP-F153</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>24.20</td>\n",
       "      <td>125.089996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>840_5088</td>\n",
       "      <td>3.542370</td>\n",
       "      <td>117.130950</td>\n",
       "      <td>02_AHL_SSP-F173</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>24.20</td>\n",
       "      <td>125.089996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>840_5153</td>\n",
       "      <td>3.598300</td>\n",
       "      <td>117.201030</td>\n",
       "      <td>02_AHL_SSP-G032</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.33</td>\n",
       "      <td>16.68</td>\n",
       "      <td>106.749997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>840_5218</td>\n",
       "      <td>3.601620</td>\n",
       "      <td>117.193570</td>\n",
       "      <td>02_AHL_SSP-G044</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.83</td>\n",
       "      <td>116.349997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>840_5283</td>\n",
       "      <td>3.601800</td>\n",
       "      <td>117.175400</td>\n",
       "      <td>02_AHL_SSP-G047</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.83</td>\n",
       "      <td>116.349997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383416 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     system:index       lat         lon               id       date  \\\n",
       "0        13_40018  3.809687   96.451939      121_APC_A21 2021-11-18   \n",
       "1        13_40056  3.811906   96.468786      121_APC_B09 2021-11-18   \n",
       "2        13_40094  3.806685   96.459393      121_APC_B14 2021-11-18   \n",
       "3        13_40132  3.855798   96.523820      121_APC_D11 2021-11-18   \n",
       "4        13_40170  3.850565   96.515498      121_APC_D15 2021-11-18   \n",
       "...           ...       ...         ...              ...        ...   \n",
       "5190     840_5023  3.562450  117.137710  02_AHL_SSP-F153 2023-01-22   \n",
       "5191     840_5088  3.542370  117.130950  02_AHL_SSP-F173 2023-01-22   \n",
       "5192     840_5153  3.598300  117.201030  02_AHL_SSP-G032 2023-01-22   \n",
       "5193     840_5218  3.601620  117.193570  02_AHL_SSP-G044 2023-01-22   \n",
       "5194     840_5283  3.601800  117.175400  02_AHL_SSP-G047 2023-01-22   \n",
       "\n",
       "      prec_3_sum  prec_7_sum  prec_30_sum  \n",
       "0           7.72        8.98   137.879997  \n",
       "1           7.72        8.98   137.879997  \n",
       "2           7.72        8.98   137.879997  \n",
       "3           6.30        7.17   120.049997  \n",
       "4           6.30        7.17   120.049997  \n",
       "...          ...         ...          ...  \n",
       "5190        0.18       24.20   125.089996  \n",
       "5191        0.18       24.20   125.089996  \n",
       "5192        0.33       16.68   106.749997  \n",
       "5193        0.10        2.83   116.349997  \n",
       "5194        0.10        2.83   116.349997  \n",
       "\n",
       "[383416 rows x 8 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_precip_sum[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_precip_sum = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_precip_sum[\"date\"] = pd.to_datetime(temp_precip_sum[\"date\"])\n",
    "temp_precip_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"Hansen_year_2023_points_2075_f.csv\"\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(explanatory_path/file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)\n",
    "hansen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_file_name = \"All_Non_temporal_points_2074.csv\"\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_df = non_temporal_df.drop(columns=[\"lat\", \"lon\"])\n",
    "non_temporal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Read extra non temporal explanatory (accumulation, distance to rivers/canals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_extra_file_name = \"All_Non_temporal_extra_points_latlon_2072.csv\"\n",
    "non_temporal_extra_df = pd.read_csv(explanatory_path/non_temporal_extra_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_extra_df = non_temporal_extra_df.drop(columns=[\"lat\", \"lon\", \"date\"])\n",
    "non_temporal_extra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_explanatory_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the extra non-temporal variables\n",
    "explanatory_df = explanatory_df.merge(non_temporal_extra_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vars = [\n",
    "    'id', 'date', 'LIA', 'VH', 'VV', 'VVVH_ratio',\n",
    "    'angle', 'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation', 'prec_3',\n",
    "    'prec_7', 'prec_30', 'elevation',\n",
    "    'aspect', 'slope', 'land_cov', 'canopy_height', 'gldas_mean',\n",
    "    'gldas_stddev', 'B3', 'B4',\n",
    "    'B5', 'B7', 'ndvi', 'ndmi', 'ndbri',\n",
    "    'distance', 'dir', 'acc',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Final step: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explanatory_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m explanatory_with_response_var \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgwl_cm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mexplanatory_df\u001b[49m[export_vars], on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add day of the year as a variable\u001b[39;00m\n\u001b[1;32m      6\u001b[0m explanatory_with_response_var[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m explanatory_with_response_var\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofyear\n",
      "\u001b[0;31mNameError\u001b[0m: name 'explanatory_df' is not defined"
     ]
    }
   ],
   "source": [
    "explanatory_with_response_var = df[[\"source\", \"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(\n",
    "    explanatory_df[export_vars], on=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "# explanatory_with_response_var.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\", index=False)\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Final step: Add the extra \"accumulated precipitation\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33420"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge explanatory_with_response_var with the one that caomes with the sum of the \n",
    "# accumulated precipitation\n",
    "import pandas as pd\n",
    "explanatory_with_response_var = pd.read_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\")\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var[\"date\"] = pd.to_datetime(explanatory_with_response_var[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_plus_precip = explanatory_with_response_var.merge(\n",
    "    temp_precip_sum[['id', 'date', \"prec_3_sum\",\"prec_7_sum\",\"prec_30_sum\"]], \n",
    "    on=[\"id\", \"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_plus_precip.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra_sum_prec.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
