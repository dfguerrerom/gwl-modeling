{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "related. \n",
    "In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_image, get_gldas, get_gpm, get_hansen, get_gpm_sum\n",
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats, get_extra_non_temporal\n",
    "from gee_scripts import init_ee\n",
    "init_ee()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267218"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords.csv', parse_dates=[\"date\"])\n",
    "assert df[\"date\"].dtype == \"datetime64[ns]\"\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235671"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requested_data = pd.read_csv(\"data/9_clean_training_data/all_training_data_with_extra_and_locations_and_precipSum.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# get the points that were not requested yet by using the date and station_id\n",
    "# requested_data[\"date\"] = requested_data[\"date\"].dt.date\n",
    "# df[\"date\"] = df[\"date\"].dt.date\n",
    "df = df[~df.set_index([\"date\", \"id\"]).index.isin(requested_data.set_index([\"date\", \"id\"]).index)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "## Set type of output\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run entirely, if we want to task the orders to GEE we'll set this variable to True\n",
    "send_task = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)\n",
    "\n",
    "# Convert them as a geodataframe and save them\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((116.84967 3.98347, 117.30926 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_id                                           geometry\n",
       "0          1  MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...\n",
       "1          2  MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...\n",
       "2          3  MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...\n",
       "3          4  MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...\n",
       "4          5  MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...\n",
       "5          6  MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...\n",
       "6          7  MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...\n",
       "7          8  MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...\n",
       "8          9  MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...\n",
       "9         10  MULTIPOLYGON (((116.84967 3.98347, 117.30926 3..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "# I have two regions, first is to only the high correlated stations and the second is all the stations\n",
    "# I will use either depending on the dataset we have selected above\n",
    "\n",
    "shp_path = Path(\"data/0_shp/\")\n",
    "region_path = \"regions_to_request_explanatory_all.gpkg\"\n",
    "\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(shp_path/region_path)\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove those date where the gwl measure is out of reasonable range\n",
    "# upper_thres = 20\n",
    "# lower_thres = -100\n",
    "\n",
    "# df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# # Get the coordinates of the individual points\n",
    "\n",
    "# unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "# len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_coors = ['138_NBR_M13','18_SBA_DTD043','18_SBA_DTP002','18_SBA_DTP025','18_SBA_DTP031','18_SBA_DTP034','18_SBA_DTP038','18_SBA_DTP054','271_RSP_H19','BRG_140301_01','BRG_140302_01','BRG_140302_02','BRG_610117_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Add the region id to each point\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"region_id\", \"geometry\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222645</th>\n",
       "      <td>BRG_150710_01</td>\n",
       "      <td>103.900168</td>\n",
       "      <td>-1.274317</td>\n",
       "      <td>POINT (103.90017 -1.27432)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245948</th>\n",
       "      <td>BRG_621107_03</td>\n",
       "      <td>114.220600</td>\n",
       "      <td>-2.654022</td>\n",
       "      <td>POINT (114.22060 -2.65402)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id         lon       lat                    geometry  \\\n",
       "222645  BRG_150710_01  103.900168 -1.274317  POINT (103.90017 -1.27432)   \n",
       "245948  BRG_621107_03  114.220600 -2.654022  POINT (114.22060 -2.65402)   \n",
       "\n",
       "        index_right  region_id  \n",
       "222645            1          2  \n",
       "245948            7          8  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Failing datasets array(['BRG_150710_01', 'BRG_621107_03'], dtype=object)\n",
    "gdf_unique_coords[gdf_unique_coords.id.isin([\"BRG_621107_03\", \"BRG_150710_01\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get temporal explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((116.84967 3.98347, 117.30926 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_id                                           geometry\n",
       "0          1  MULTIPOLYGON (((96.37854 4.01317, 96.76923 3.9...\n",
       "1          2  MULTIPOLYGON (((102.96446 -0.63790, 104.82488 ...\n",
       "2          3  MULTIPOLYGON (((140.00836 -7.80760, 140.75163 ...\n",
       "3          4  MULTIPOLYGON (((105.23245 -2.56075, 105.62785 ...\n",
       "4          5  MULTIPOLYGON (((101.59551 1.61281, 101.45686 0...\n",
       "5          6  MULTIPOLYGON (((100.69365 2.01094, 100.81080 2...\n",
       "6          7  MULTIPOLYGON (((108.80424 1.60848, 109.83126 1...\n",
       "7          8  MULTIPOLYGON (((110.98152 -2.86934, 114.00610 ...\n",
       "8          9  MULTIPOLYGON (((132.99060 -0.68691, 133.43736 ...\n",
       "9         10  MULTIPOLYGON (((116.84967 3.98347, 117.30926 3..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>POINT (117.00775 3.93776)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>02_AHL_SBG-B101</td>\n",
       "      <td>117.010120</td>\n",
       "      <td>3.931860</td>\n",
       "      <td>POINT (117.01012 3.93186)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>02_AHL_SBG-B103</td>\n",
       "      <td>117.005210</td>\n",
       "      <td>3.926090</td>\n",
       "      <td>POINT (117.00521 3.92609)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>02_AHL_SBG-C003</td>\n",
       "      <td>117.145430</td>\n",
       "      <td>3.903400</td>\n",
       "      <td>POINT (117.14543 3.90340)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>02_AHL_SBG-C006</td>\n",
       "      <td>117.148320</td>\n",
       "      <td>3.919380</td>\n",
       "      <td>POINT (117.14832 3.91938)</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266127</th>\n",
       "      <td>ij2</td>\n",
       "      <td>114.022576</td>\n",
       "      <td>-2.573375</td>\n",
       "      <td>POINT (114.02258 -2.57337)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266206</th>\n",
       "      <td>jambi1</td>\n",
       "      <td>103.589975</td>\n",
       "      <td>-1.238478</td>\n",
       "      <td>POINT (103.58997 -1.23848)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266449</th>\n",
       "      <td>kalbar1</td>\n",
       "      <td>109.394853</td>\n",
       "      <td>-0.210225</td>\n",
       "      <td>POINT (109.39485 -0.21022)</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266842</th>\n",
       "      <td>kalteng1</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>POINT (114.05813 -2.31973)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267180</th>\n",
       "      <td>kecil1</td>\n",
       "      <td>113.805611</td>\n",
       "      <td>-2.856089</td>\n",
       "      <td>POINT (113.80561 -2.85609)</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2074 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id         lon       lat                    geometry  \\\n",
       "0       02_AHL_SBG-B076  117.007750  3.937760   POINT (117.00775 3.93776)   \n",
       "65      02_AHL_SBG-B101  117.010120  3.931860   POINT (117.01012 3.93186)   \n",
       "130     02_AHL_SBG-B103  117.005210  3.926090   POINT (117.00521 3.92609)   \n",
       "195     02_AHL_SBG-C003  117.145430  3.903400   POINT (117.14543 3.90340)   \n",
       "260     02_AHL_SBG-C006  117.148320  3.919380   POINT (117.14832 3.91938)   \n",
       "...                 ...         ...       ...                         ...   \n",
       "266127              ij2  114.022576 -2.573375  POINT (114.02258 -2.57337)   \n",
       "266206           jambi1  103.589975 -1.238478  POINT (103.58997 -1.23848)   \n",
       "266449          kalbar1  109.394853 -0.210225  POINT (109.39485 -0.21022)   \n",
       "266842         kalteng1  114.058131 -2.319728  POINT (114.05813 -2.31973)   \n",
       "267180           kecil1  113.805611 -2.856089  POINT (113.80561 -2.85609)   \n",
       "\n",
       "        index_right  region_id  \n",
       "0                 9         10  \n",
       "65                9         10  \n",
       "130               9         10  \n",
       "195               9         10  \n",
       "260               9         10  \n",
       "...             ...        ...  \n",
       "266127            7          8  \n",
       "266206            1          2  \n",
       "266449            6          7  \n",
       "266842            7          8  \n",
       "267180            7          8  \n",
       "\n",
       "[2074 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1_Precipitation_sum_non_resample_at_all_region_1_dates_444_points_24_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_2_dates_1767_points_148_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_3_dates_421_points_1_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_4_dates_986_points_348_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_5_dates_1799_points_718_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_6_dates_430_points_43_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_7_dates_1219_points_477_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_8_dates_1664_points_221_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat\n",
      "Exported 1_Precipitation_sum_non_resample_at_all_region_10_dates_780_points_77_with_date_lon_lat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_selectors = [\"system:index\", \"lat\", \"lon\", \"id\", \"date\"]\n",
    "s1_selectors = [\"LIA\", \"VH\", \"VV\", \"VVVH_ratio\", \"angle\"]\n",
    "gldas_selectors = ['sm_1', 'sm_3', 'sm_7', 'sm_30']\n",
    "gpm_selectors = ['precipitation', 'prec_3', 'prec_7', 'prec_30']\n",
    "gpm_selectors_sum = ['prec_3_sum', 'prec_7_sum', 'prec_30_sum']\n",
    "\n",
    "def get_temporal_explanatory(region_id):\n",
    "    \"\"\"Get the explanatory temporal based variables\"\"\"\n",
    "\n",
    "    region = gdf_regions[gdf_regions.region_id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "    dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.region_id == region_id].id.unique())].date.unique()\n",
    "    points = gdf_unique_coords[gdf_unique_coords.region_id == region_id][[\"id\", \"geometry\", \"lat\", \"lon\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # print(len(dates), len(points))\n",
    "    # Convert to ee elements\n",
    "\n",
    "    ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "    ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "    def get_sources(date_feature):\n",
    "\n",
    "        date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "        s1_composite = get_s1_image(date_range, ee_region)\n",
    "\n",
    "        return s1_composite.set({\n",
    "         \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "         \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "         })\n",
    "\n",
    "    def reduce_composite(composite):\n",
    "\n",
    "        # Filter the extra data with the matching date\n",
    "        date = composite.get(\"date\")\n",
    "        date_range = ee.Date(date).getRange('day')\n",
    "\n",
    "        gldas_composite = get_gldas(date_range, ee_region)\n",
    "        gpm_composite = get_gpm(date_range, ee_region)\n",
    "        gpm_sum_composite = get_gpm_sum(date_range, ee_region)\n",
    "\n",
    "        composite = (ee.Image(composite)\n",
    "            .addBands(gldas_composite)\n",
    "            .addBands(gpm_composite)\n",
    "            .addBands(gpm_sum_composite)\n",
    "        )\n",
    "\n",
    "        return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "        }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "         \"date\" : date\n",
    "        }))\n",
    "\n",
    "\n",
    "    task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "    )\n",
    "\n",
    "    # task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "    task_name = f\"1_Precipitation_sum_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\"\"\n",
    "\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "      \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors + gpm_selectors_sum\n",
    "\n",
    "    })\n",
    "\n",
    "    # Uncoment to start the task\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "send_task = True\n",
    "[get_temporal_explanatory(region_id) for region_id in gdf_regions.region_id.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_selectors = [\"year\", \"B3\",\"B4\",\"B5\",\"B7\",\"ndvi\",\"ndmi\",\"ndbri\"]\n",
    "\n",
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013] )\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "    points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "    image = get_hansen(year)\n",
    "\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "    \n",
    "    task_name = f\"Hansen_year_{year}_points_{len(points)}_f\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": f\"Hansen_year_{year}_points_{len(points)}_f\",\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset is not too computational expensive, so we are not forced to chunk it\n",
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id\", \"geometry\", \"lat\", \"lon\"]].rename(columns={\"id\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    ")\n",
    "composite.bandNames().getInfo()\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "task_name = f\"All_Non_temporal_points_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['elevation', 'aspect', 'slope', 'land_cov', 'canopy_height', \"gldas_mean\", \"gldas_stddev\"]\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Extra Non temporal explanatory variables (others)\n",
    "\n",
    "This data comes from https://code.earthengine.google.com/6c3eeb929a5ee8a42f55234b58796c0a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distance', 'dir', 'acc', 'land_forms']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite = get_extra_non_temporal()\n",
    "composite.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1_All_Non_temporal_extra_points_latlon_2074\n"
     ]
    }
   ],
   "source": [
    "phu = ee.FeatureCollection(\n",
    "    \"users/marortpab/FAO/SEPAL/2023_trainings/smm/AOI__Province__865_PHUs__INDONESIA\"\n",
    ")\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['distance']))\n",
    "\n",
    "task_name = f\"1_All_Non_temporal_extra_points_latlon_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['distance', 'dir', 'acc']\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "dataset = \"all\"\n",
    "temporal_file_names_groups = {\n",
    "    \"all\" : [\n",
    "        \"All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_8_dates_1671_points_220_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "# temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I modified the notebook on the 31/05/2024 to include the sum of the precipitation\n",
    "temporal_precip_sum = {\n",
    "    \"all\" : [\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_3_dates_362_points_1_with_date_lon_lat.csv\", # Using this there's only 362 where the other has 479\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_5_dates_1796_points_718_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_7_dates_1273_points_477_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_8_dates_1671_points_219_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_precip_sum[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_precip_sum = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_precip_sum[\"date\"] = pd.to_datetime(temp_precip_sum[\"date\"])\n",
    "temp_precip_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I modified the notebook on the 12/06/2024 to include the stations that we removed at the beginning\n",
    "# by applying the threshold of the gwl (20 and -100)\n",
    "missed_stations = {\n",
    "    \"all\" : [\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_1_dates_444_points_24_with_date_lon_lat.csv\",        \"1_Precipitation_sum_non_resample_at_all_region_2_dates_1767_points_148_with_date_lon_lat.csv\",\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_3_dates_421_points_1_with_date_lon_lat.csv\",        \"1_Precipitation_sum_non_resample_at_all_region_4_dates_986_points_348_with_date_lon_lat.csv\",\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_5_dates_1799_points_718_with_date_lon_lat.csv\",\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_6_dates_430_points_43_with_date_lon_lat.csv\",        \"1_Precipitation_sum_non_resample_at_all_region_7_dates_1219_points_477_with_date_lon_lat.csv\",\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_8_dates_1664_points_221_with_date_lon_lat.csv\",\n",
    "        \"1_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",        \"1_Precipitation_sum_non_resample_at_all_region_10_dates_780_points_77_with_date_lon_lat.csv\",\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "missed_temporal_file_names = missed_stations[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "missed_temp = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in missed_temporal_file_names\n",
    "        ]\n",
    ")\n",
    "missed_temp[\"date\"] = pd.to_datetime(missed_temp[\"date\"])\n",
    "missed_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"Hansen_year_2023_points_2075_f.csv\"\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(explanatory_path/file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)\n",
    "hansen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_file_name = \"All_Non_temporal_points_2074.csv\"\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_df = non_temporal_df.drop(columns=[\"lat\", \"lon\"])\n",
    "non_temporal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Read extra non temporal explanatory (accumulation, distance to rivers/canals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_extra_file_name = \"All_Non_temporal_extra_points_latlon_2072.csv\"\n",
    "non_temporal_extra_df = pd.read_csv(explanatory_path/non_temporal_extra_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_extra_df = non_temporal_extra_df.drop(columns=[\"lat\", \"lon\", \"date\"])\n",
    "non_temporal_extra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_explanatory_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the extra non-temporal variables\n",
    "explanatory_df = explanatory_df.merge(non_temporal_extra_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vars = [\n",
    "    'id', 'date', 'LIA', 'VH', 'VV', 'VVVH_ratio',\n",
    "    'angle', 'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation', 'prec_3',\n",
    "    'prec_7', 'prec_30', 'elevation',\n",
    "    'aspect', 'slope', 'land_cov', 'canopy_height', 'gldas_mean',\n",
    "    'gldas_stddev', 'B3', 'B4',\n",
    "    'B5', 'B7', 'ndvi', 'ndmi', 'ndbri',\n",
    "    'distance', 'dir', 'acc',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Final step: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explanatory_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m explanatory_with_response_var \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgwl_cm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mexplanatory_df\u001b[49m[export_vars], on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add day of the year as a variable\u001b[39;00m\n\u001b[1;32m      6\u001b[0m explanatory_with_response_var[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m explanatory_with_response_var\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofyear\n",
      "\u001b[0;31mNameError\u001b[0m: name 'explanatory_df' is not defined"
     ]
    }
   ],
   "source": [
    "explanatory_with_response_var = df[[\"source\", \"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(\n",
    "    explanatory_df[export_vars], on=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "# explanatory_with_response_var.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\", index=False)\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Final step: Add the extra \"accumulated precipitation\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33420"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge explanatory_with_response_var with the one that caomes with the sum of the \n",
    "# accumulated precipitation\n",
    "import pandas as pd\n",
    "explanatory_with_response_var = pd.read_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\")\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var[\"date\"] = pd.to_datetime(explanatory_with_response_var[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_plus_precip = explanatory_with_response_var.merge(\n",
    "    temp_precip_sum[['id', 'date', \"prec_3_sum\",\"prec_7_sum\",\"prec_30_sum\"]], \n",
    "    on=[\"id\", \"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_plus_precip.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra_sum_prec.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
