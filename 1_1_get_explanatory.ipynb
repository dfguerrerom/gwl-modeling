{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "related. \n",
    "In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_collection, get_gldas, get_gpm, get_hansen, get_gpm_sum\n",
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats, get_extra_non_temporal\n",
    "from gee_scripts import init_ee\n",
    "init_ee()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords_plus_bosf.csv', parse_dates=[\"date\"])\n",
    "assert df[\"date\"].dtype == \"datetime64[ns]\"\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########33 THIS IS TEMPORARY #############\n",
    "\n",
    "# # For the second iteration I will only use the missing data from bosf\n",
    "\n",
    "# missing_bosf = pd.read_csv(\"data/7_training_data/bosf/missing_points_dates.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# # concatenate date and id to get unique id\n",
    "# df[\"date_id\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \"_\" + df[\"id\"].astype(str)\n",
    "\n",
    "# # Filter in the missing bosf data\n",
    "# df = df[df[\"date_id\"].isin(missing_bosf[\"date_id\"])]\n",
    "# len(df)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.source==\"bosf_NASA\"].drop_duplicates(subset=[\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.gwl_cm < 400].gwl_cm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "## Set type of output\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run entirely, if we want to task the orders to GEE we'll set this variable to True\n",
    "send_task = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them as a geodataframe and save them\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "# I have two regions, first is to only the high correlated stations and the second is all the stations\n",
    "# I will use either depending on the dataset we have selected above\n",
    "\n",
    "shp_path = Path(\"data/0_shp/\")\n",
    "region_path = \"bosf_region.shp\"\n",
    "\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(shp_path/region_path)\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove those date where the gwl measure is out of reasonable range\n",
    "# upper_thres = 20\n",
    "# lower_thres = -100\n",
    "\n",
    "# df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# # Get the coordinates of the individual points\n",
    "\n",
    "# unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "# len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Add the region id to each point\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"region_id\", \"geometry\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get temporal explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base selectors\n",
    "\n",
    "base_selectors = [\"system:index\", \"lat\", \"lon\", \"id\", \"date\"]\n",
    "s1_selectors = [\"LIA\", \"VH\", \"VV\", \"VVVH_ratio\", \"angle\"]\n",
    "gldas_selectors = ['sm_1', 'sm_3', 'sm_7', 'sm_30']\n",
    "gpm_selectors = ['precipitation', 'prec_3', 'prec_7', 'prec_30']\n",
    "gpm_selectors_sum = ['prec_3_sum', 'prec_7_sum', 'prec_30_sum']\n",
    "hansen_selectors = [\"year\", \"B3\",\"B4\",\"B5\",\"B7\",\"ndvi\",\"ndmi\",\"ndbri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_explanatory(region_id, mode: Literal[\"strict\", \"extra\"]=\"strict\", position=1):\n",
    "    \"\"\"Get the explanatory temporal based variables.\n",
    "    \n",
    "    Parameters:\n",
    "    region_id: int\n",
    "        The region id to get the data for\n",
    "    mode: Literal[\"strict\", \"extra\"]\n",
    "        strict: Only get the data for the exact date\n",
    "        extra: Get the closest image to the date and adds the time difference as a variable\n",
    "    position: int\n",
    "        The position of the image selected in the collection, starts at 1\n",
    "    \"\"\"\n",
    "\n",
    "    region = gdf_regions[gdf_regions.region_id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "    dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.region_id == region_id].id.unique())].date.unique()\n",
    "    points = gdf_unique_coords[gdf_unique_coords.region_id == region_id][[\"id\", \"geometry\", \"lat\", \"lon\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "    if mode == \"extra\":\n",
    "        base_selectors + [\"time_difference\"]\n",
    "\n",
    "    # print(len(dates), len(points))\n",
    "    # Convert to ee elements\n",
    "\n",
    "    ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "    ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "    def get_sources(date_feature):\n",
    "\n",
    "        if mode==\"strict\":\n",
    "            date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "            s1_composite = get_s1_collection(date_range, ee_region)\n",
    "\n",
    "            return s1_composite.set({\n",
    "            \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "            \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "            })\n",
    "        \n",
    "        elif mode==\"extra\":\n",
    "\n",
    "            requested_date = ee.Date(date_feature.get(\"date\"))\n",
    "            # Define a time window to search for images, e.g., +/- 30 days\n",
    "            start_date = requested_date.advance(-30, 'day')\n",
    "            end_date = requested_date.advance(30, 'day')\n",
    "\n",
    "            # Get all S1 images in the time window\n",
    "            s1_collection = get_s1_collection(ee.DateRange(start_date, end_date), ee_region)\n",
    "\n",
    "            # Function to compute absolute difference in days between image date and requested date\n",
    "            def compute_abs_difference(image):\n",
    "                diff = ee.Number(image.date().difference(requested_date, 'day')).abs()\n",
    "                return image.set('time_difference', diff)\n",
    "\n",
    "            # Map over the collection to compute time difference\n",
    "            s1_collection = s1_collection.map(compute_abs_difference)\n",
    "\n",
    "            # Sort the collection by time difference\n",
    "            sorted_collection = s1_collection.sort('time_difference')\n",
    "\n",
    "            # Get the date of the position image in the collection\n",
    "\n",
    "            selected_image = ee.Image(sorted_collection.toList(sorted_collection.size()).get(position-1))\n",
    "\n",
    "            selected_image_date = selected_image.date()\n",
    "            selected_image_time_difference = selected_image.get('time_difference')\n",
    "\n",
    "            # Filter the collection to images that have the same date as the closest image\n",
    "            images_same_date = s1_collection.filterDate(selected_image_date, selected_image_date.advance(1, 'day'))\n",
    "\n",
    "            # Mosaic the images\n",
    "            s1_image = images_same_date.median()\n",
    "\n",
    "            return s1_image.set({\n",
    "                \"numberOfBands\": s1_image.bandNames().size(),\n",
    "                \"date\": requested_date,\n",
    "                \"time_difference\": selected_image_time_difference\n",
    "            })\n",
    "\n",
    "\n",
    "    def reduce_composite(composite):\n",
    "\n",
    "        # Filter the extra data with the matching date\n",
    "        date = composite.get(\"date\")\n",
    "        date_range = ee.Date(date).getRange('day')\n",
    "        time_difference = composite.get(\"time_difference\")\n",
    "\n",
    "        gldas_composite = get_gldas(date_range, ee_region)\n",
    "        gpm_composite = get_gpm(date_range, ee_region)\n",
    "        gpm_sum_composite = get_gpm_sum(date_range, ee_region)\n",
    "\n",
    "        composite = (ee.Image(composite)\n",
    "            .addBands(gldas_composite)\n",
    "            .addBands(gpm_composite)\n",
    "            .addBands(gpm_sum_composite)\n",
    "        )\n",
    "\n",
    "        return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "        }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "            \"date\" : date,\n",
    "            \"time_difference\": time_difference,\n",
    "        }))\n",
    "\n",
    "\n",
    "    task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "    )\n",
    "\n",
    "    # task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "    task_name = f\"1_Precipitation_sum_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}bosf_1ND_missing\"\"\"\n",
    "\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "      \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors + gpm_selectors_sum\n",
    "\n",
    "    })\n",
    "\n",
    "    # Uncoment to start the task\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "\n",
    "mode = \"extra\"\n",
    "\n",
    "if mode==\"extra\":\n",
    "    base_selectors = list(set(base_selectors + [\"time_difference\"]))\n",
    "\n",
    "# Region_id 20 is the bosf region, use mode=\"extra\" to get the closest image to the date\n",
    "# [get_temporal_explanatory(region_id, mode=mode, position=2) for region_id in gdf_regions.region_id.unique() if region_id in [20]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013] )\n",
    "\n",
    "points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "# This is only for the bosf region\n",
    "points = gdf_unique_coords[gdf_unique_coords.region_id == 20][[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "no_points = ee_points.size().getInfo()\n",
    "\n",
    "print(f\"Processing points {no_points}\")\n",
    "\n",
    "for year in [2020]:\n",
    "    \n",
    "    # The dataset for 2020 is not available ,try with get_landsat_mosaic\n",
    "    image = get_hansen(year)\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "\n",
    "    task_name = f\"Hansen_year_{year}_points_{no_points}_f_bosf\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": task_name,\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "\n",
    "    # not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is not too computational expensive, so we are not forced to chunk it\n",
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "\n",
    "# Below is just to use bosf region\n",
    "region = gdf_regions[gdf_regions.region_id==20].to_crs(\"EPSG:4326\")[:]\n",
    "\n",
    "\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id\", \"geometry\", \"lat\", \"lon\"]].rename(columns={\"id\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__).filterBounds(ee_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    ")\n",
    "composite.bandNames().getInfo()\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "no_points = ee_points.size().getInfo()\n",
    "task_name = f\"All_Non_temporal_points_{no_points}_bosf\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['elevation', 'aspect', 'slope', 'land_cov', 'canopy_height', \"gldas_mean\", \"gldas_stddev\"]\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "# not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Extra Non temporal explanatory variables (others)\n",
    "\n",
    "This data comes from https://code.earthengine.google.com/6c3eeb929a5ee8a42f55234b58796c0a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_points = ee_points.size().getInfo()\n",
    "task_name = f\"1_All_Non_temporal_extra_points_latlon_{no_points}_bosf\"\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['distance']))\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['distance', 'dir', 'acc']\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "# not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory_path = Path(\"data/7_training_data/\")\n",
    "# dataset = \"all\"\n",
    "# temporal_file_names_groups = {\n",
    "#     \"all\" : [\n",
    "#         \"All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_8_dates_1671_points_220_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "# temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# # get and concatenate all the dataframes\n",
    "# temp_explanatory_dfs = pd.concat([\n",
    "#             pd.read_csv(explanatory_path/file_name, parse_dates=[\"date\"])\n",
    "#             for file_name \n",
    "#             in temporal_file_names\n",
    "#         ], \n",
    "# )\n",
    "\n",
    "# temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "# temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "dataset = \"bosf\"\n",
    "\n",
    "# I modified the notebook on the 31/05/2024 to include the sum of the precipitation\n",
    "temporal_precip_sum = {\n",
    "    \"all\" : [\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_3_dates_362_points_1_with_date_lon_lat.csv\", # Using this there's only 362 where the other has 479\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_5_dates_1796_points_718_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_7_dates_1273_points_477_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_8_dates_1671_points_219_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ],\n",
    "    \"bosf\" : [\n",
    "        \"bosf/1_Precipitation_sum_non_resample_at_all_region_20_dates_644_points_381_with_date_lon_lat_bosf.csv\",\n",
    "        \"bosf/1_Precipitation_sum_non_resample_at_all_region_20_dates_115_points_341bosf_1ND_missing.csv\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_precip_sum[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "temp_explanatory_dfs\n",
    "\n",
    "print(len(temp_explanatory_dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/bosf/\")\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"Hansen_year_2023_points_2075_f.csv\"\n",
    "]\n",
    "\n",
    "# This is for bosf\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2018_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2019_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2020_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2021_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2022_points_381_f_bosf.csv\",\n",
    "    \"Hansen_year_2023_points_381_f_bosf.csv\"\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(explanatory_path/file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "# non_temporal_file_name = \"All_Non_temporal_points_2074.csv\"\n",
    "\n",
    "# for bosf\n",
    "non_temporal_file_name = \"All_Non_temporal_points_381_bosf.csv\"\n",
    "\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_df = non_temporal_df.drop(columns=[\"lat\", \"lon\", \"date\", \"time_difference\"])\n",
    "non_temporal_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Read extra non temporal explanatory (accumulation, distance to rivers/canals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "# non_temporal_extra_file_name = \"All_Non_temporal_extra_points_latlon_2072.csv\"\n",
    "\n",
    "# for bosf data\n",
    "non_temporal_extra_file_name = \"1_All_Non_temporal_extra_points_latlon_381_bosf.csv\"\n",
    "\n",
    "non_temporal_extra_df = pd.read_csv(explanatory_path/non_temporal_extra_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_extra_df = non_temporal_extra_df.drop(columns=[\"lat\", \"lon\", \"date\"])\n",
    "non_temporal_extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the extra non-temporal variables\n",
    "explanatory_df = explanatory_df.merge(non_temporal_extra_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vars = [\n",
    "    'id', 'date', 'LIA', 'VH', 'VV', 'VVVH_ratio',\n",
    "    'angle', 'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation', 'prec_3',\n",
    "    'prec_7', 'prec_30', 'elevation',\n",
    "    'aspect', 'slope', 'land_cov', 'canopy_height', 'gldas_mean',\n",
    "    'gldas_stddev', 'B3', 'B4',\n",
    "    'B5', 'B7', 'ndvi', 'ndmi', 'ndbri',\n",
    "    'distance', 'dir', 'acc',\n",
    "] + [\"time_difference\"] + ['prec_3_sum', 'prec_7_sum', 'prec_30_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Final step: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var = df[[\"source\", \"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(\n",
    "    explanatory_df[export_vars], on=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "# explanatory_with_response_var.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\", index=False)\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lenght of base_df: \", len(explanatory_with_response_var))\n",
    "# Drop duplicates on id and date\n",
    "explanatory_with_response_var = explanatory_with_response_var.drop_duplicates(subset=[\"id\", \"date\"])\n",
    "print(\"lenght of explanatory_with_response_var after dropping duplicates: \", len(explanatory_with_response_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data\n",
    "explanatory_with_response_var.to_csv(\"data/7_training_data/bosf/explanatory_with_response_var_and_source_extra_sum_prec_bosf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Final step: Add the extra \"accumulated precipitation\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont' have to do this for BOSF because I requested them at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge explanatory_with_response_var with the one that caomes with the sum of the \n",
    "# # accumulated precipitation\n",
    "# explanatory_with_response_var = pd.read_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\", parse_dates=[\"date\"])\n",
    "# len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory_with_response_plus_precip = explanatory_with_response_var.merge(\n",
    "#     temp_precip_sum[['id', 'date', \"prec_3_sum\",\"prec_7_sum\",\"prec_30_sum\"]], \n",
    "#     on=[\"id\", \"date\"]\n",
    "# )\n",
    "# len(explanatory_with_response_plus_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explanatory_with_response_plus_precip.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra_sum_prec.csv\", index=False)\n",
    "\n",
    "# explanatory_with_response_var_and_source_extra_sum_prec = pd.read_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra_sum_prec.csv\", parse_dates=[\"date\"])\n",
    "# len(explanatory_with_response_var_and_source_extra_sum_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.gwl_cm < 400) & (df.date > \"2019-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.plots import plot_ts\n",
    "df = df[(df.gwl_cm < 500) & (df.date > \"2019-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts(df, \"gwl_cm\", title=\"explain df plus precip sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts(explanatory_with_response_var_and_source_extra_sum_prec, \"gwl_cm\", title=\"explain df plus precip sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts(explanatory_with_response_var, \"gwl_cm\", title=\"explain df\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
