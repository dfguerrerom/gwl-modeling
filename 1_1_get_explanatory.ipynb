{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "related. \n",
    "In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_collection, get_gldas, get_gpm, get_hansen, get_gpm_sum\n",
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats, get_extra_non_temporal\n",
    "from gee_scripts import init_ee\n",
    "from gee_scripts.parameters import best_kalimantan_phus, bad_stations\n",
    "\n",
    "init_ee()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords_plus_bosf_and_spatial.csv', parse_dates=[\"date\"])\n",
    "assert df[\"date\"].dtype == \"datetime64[ns]\"\n",
    "print(len(df))\n",
    "df.columns, df.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.phu_id.isin(best_kalimantan_phus)) & (~df.id.isin(bad_stations)) | (df.source == \"bosf_NASA\")]\n",
    "print(len(df))\n",
    "\n",
    "# # subset by unique id and date and show the lenght  \n",
    "\n",
    "# Filter only by bosf\n",
    "# df = df[df.source == \"bosf_NASA\"]\n",
    "\n",
    "\n",
    "print(len(df.drop_duplicates(subset=[\"id\", \"date\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########33 THIS IS TEMPORARY #############\n",
    "\n",
    "# For the second iteration I will only use the missing data from bosf\n",
    "\n",
    "# missing_bosf = pd.read_csv(\"data/7_training_data/bosf/missing_points_dates.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# # concatenate date and id to get unique id\n",
    "# df[\"date_id\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \"_\" + df[\"id\"].astype(str)\n",
    "\n",
    "# # Filter in the missing bosf data\n",
    "# df = df[df[\"date_id\"].isin(missing_bosf[\"date_id\"])]\n",
    "# len(df)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "## Set type of output\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run entirely, if we want to task the orders to GEE we'll set this variable to True\n",
    "send_task = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them as a geodataframe and save them\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "shp_path = Path(\"data/0_shp/\")\n",
    "region_path = \"regions_to_request_explanatory_all.gpkg\"\n",
    "\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(shp_path/region_path)\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove those date where the gwl measure is out of reasonable range\n",
    "# upper_thres = 20\n",
    "# lower_thres = -100\n",
    "\n",
    "# df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# # Get the coordinates of the individual points\n",
    "\n",
    "# unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "# len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Add the region id to each point\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"region_id\", \"geometry\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdf_unique_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set selectors\n",
    "from gee_scripts.parameters import base_selectors, s1_selectors, gldas_selectors, gpm_selectors, gpm_selectors_sum, hansen_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_explanatory(region_id, mode: Literal[\"strict\", \"extra\"]=\"strict\", position=1):\n",
    "    \"\"\"Get the explanatory temporal based variables.\n",
    "    \n",
    "    Parameters:\n",
    "    region_id: int\n",
    "        The region id to get the data for\n",
    "    mode: Literal[\"strict\", \"extra\"]\n",
    "        strict: Only get the data for the exact date\n",
    "        extra: Get the closest image to the date and adds the time difference as a variable\n",
    "    position: int\n",
    "        The position of the image selected in the collection, starts at 1\n",
    "    \"\"\"\n",
    "\n",
    "    region = gdf_regions[gdf_regions.region_id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "    dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.region_id == region_id].id.unique())].date.unique()\n",
    "    points = gdf_unique_coords[gdf_unique_coords.region_id == region_id][[\"id\", \"geometry\", \"lat\", \"lon\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "    if mode == \"extra\":\n",
    "        base_selectors + [\"time_difference\"]\n",
    "\n",
    "    # print(len(dates), len(points))\n",
    "    # Convert to ee elements\n",
    "\n",
    "    ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "    ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "    print(f\"Region {region_id} has {len(dates)} dates and {len(points)} points\")\n",
    "\n",
    "    def get_sources(date_feature):\n",
    "\n",
    "        if mode==\"strict\":\n",
    "            date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "            s1_composite = get_s1_collection(date_range, ee_region).median()\n",
    "\n",
    "            return s1_composite.set({\n",
    "            \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "            \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "            })\n",
    "        \n",
    "        elif mode==\"extra\":\n",
    "            requested_date = ee.Date(date_feature.get(\"date\"))\n",
    "            # Define a time window to search for images, e.g., +/- 30 days\n",
    "            start_date = requested_date.advance(-30, 'day')\n",
    "            end_date = requested_date.advance(30, 'day')\n",
    "\n",
    "            # Get all S1 images in the time window\n",
    "            s1_collection = get_s1_collection(ee.DateRange(start_date, end_date), ee_region)\n",
    "\n",
    "            # Function to compute absolute difference in days between image date and requested date\n",
    "            def compute_abs_difference(image):\n",
    "                diff = ee.Number(image.date().difference(requested_date, 'day')).abs()\n",
    "                return image.set('time_difference', diff)\n",
    "\n",
    "            # Map over the collection to compute time difference\n",
    "            s1_collection = s1_collection.map(compute_abs_difference)\n",
    "\n",
    "            # Sort the collection by time difference\n",
    "            sorted_collection = s1_collection.sort('time_difference')\n",
    "\n",
    "            # Get the date of the position image in the collection\n",
    "\n",
    "            selected_image = ee.Image(sorted_collection.toList(sorted_collection.size()).get(position-1))\n",
    "\n",
    "            selected_image_date = selected_image.date()\n",
    "            selected_image_time_difference = selected_image.get('time_difference')\n",
    "\n",
    "            # Filter the collection to images that have the same date as the closest image\n",
    "            images_same_date = s1_collection.filterDate(selected_image_date, selected_image_date.advance(1, 'day'))\n",
    "\n",
    "            # Mosaic the images\n",
    "            s1_image = images_same_date.median()\n",
    "\n",
    "            return s1_image.set({\n",
    "                \"numberOfBands\": s1_image.bandNames().size(),\n",
    "                \"date\": requested_date,\n",
    "                \"time_difference\": selected_image_time_difference\n",
    "            })\n",
    "\n",
    "\n",
    "    def reduce_composite(composite):\n",
    "\n",
    "        # Filter the extra data with the matching date\n",
    "        date = composite.get(\"date\")\n",
    "        date_range = ee.Date(date).getRange('day')\n",
    "        time_difference = composite.get(\"time_difference\")\n",
    "\n",
    "        gldas_composite = get_gldas(date_range, ee_region)\n",
    "        gpm_composite = get_gpm(date_range, ee_region)\n",
    "        gpm_sum_composite = get_gpm_sum(date_range, ee_region)\n",
    "\n",
    "        composite = (ee.Image(composite)\n",
    "            .addBands(gldas_composite)\n",
    "            .addBands(gpm_composite)\n",
    "            .addBands(gpm_sum_composite)\n",
    "        )\n",
    "\n",
    "        return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "        }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "            \"date\" : date,\n",
    "            \"time_difference\": time_difference,\n",
    "        }))\n",
    "\n",
    "\n",
    "    task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "    )\n",
    "\n",
    "    # task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "    task_name = f\"all_temporal_extended_{region_id}_dates_{len(dates)}_points_{len(points)}brgm_pkeg\"\"\"\n",
    "\n",
    "    if mode==\"extra\":\n",
    "        task_name = task_name + f\"_position_{position}\"\n",
    "\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "      \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors + gpm_selectors_sum\n",
    "\n",
    "    })\n",
    "\n",
    "    # Uncoment to start the task\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "\n",
    "\n",
    "\n",
    "#### SET PARAMETERS ####\n",
    "mode:Literal[\"estrict\", \"extra\"] = \"strict\"\n",
    "\n",
    "if mode==\"extra\":\n",
    "    base_selectors = list(set(base_selectors + [\"time_difference\"]))\n",
    "\n",
    "# Region_id 20 is the bosf region, use mode=\"extra\" to get the closest image to the date\n",
    "[get_temporal_explanatory(region_id, mode=mode, position=3) for region_id in gdf_regions.region_id.unique() if region_id in [8]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013])\n",
    "points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "# This is only for the bosf region\n",
    "points = gdf_unique_coords[gdf_unique_coords.region_id == 8][[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "print(len(points))\n",
    "\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "no_points = ee_points.size().getInfo()\n",
    "\n",
    "print(f\"Processing points {no_points}\")\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    # The dataset for 2020 is not available ,try with get_landsat_mosaic\n",
    "    image = get_hansen(year)\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "\n",
    "\n",
    "\n",
    "    task_name = f\"Hansen_year_{year}_points_{no_points}_f_bosf\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": task_name,\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is not too computational expensive, so we are not forced to chunk it\n",
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "\n",
    "# Below is just to use bosf region\n",
    "region = gdf_regions[gdf_regions.region_id==8].to_crs(\"EPSG:4326\")[:]\n",
    "\n",
    "\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id\", \"geometry\", \"lat\", \"lon\"]].rename(columns={\"id\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__).filterBounds(ee_region)\n",
    "\n",
    "print(ee_points.size().getInfo())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.parameters import non_temporal_vars\n",
    "\n",
    "# Remove time_difference from the base selectors\n",
    "base_selectors = [selector for selector in base_selectors if selector != \"time_difference\"]\n",
    "\n",
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    "        .addBands(get_extra_non_temporal())\n",
    ")\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "no_points = ee_points.size().getInfo()\n",
    "task_name = f\"All_Non_temporal_points_{no_points}_bosf_and_all\"\n",
    "\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + non_temporal_vars\n",
    "})\n",
    "\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory_path = Path(\"data/7_training_data/\")\n",
    "# dataset = \"all\"\n",
    "# temporal_file_names_groups = {\n",
    "#     \"all\" : [\n",
    "#         \"All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_8_dates_1671_points_220_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "#         \"All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "# temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# # get and concatenate all the dataframes\n",
    "# temp_explanatory_dfs = pd.concat([\n",
    "#             pd.read_csv(explanatory_path/file_name, parse_dates=[\"date\"])\n",
    "#             for file_name \n",
    "#             in temporal_file_names\n",
    "#         ], \n",
    "# )\n",
    "\n",
    "# temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "# temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "dataset = \"brg_pkeg_time_diff\"\n",
    "\n",
    "# I modified the notebook on the 31/05/2024 to include the sum of the precipitation\n",
    "temporal_precip_sum = {\n",
    "    \"all\" : [\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_3_dates_362_points_1_with_date_lon_lat.csv\", # Using this there's only 362 where the other has 479\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_5_dates_1796_points_718_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_7_dates_1273_points_477_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_8_dates_1671_points_219_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"0_Precipitation_sum_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ],\n",
    "    \"bosf\" : [\n",
    "        \"bosf/all_temporal_extended_20_dates_644_points_381bosf_it_0.csv\",\n",
    "        \"bosf/all_temporal_extended_20_dates_644_points_381bosf_it_1_filtered.csv\",\n",
    "    ],\n",
    "    \"brg_pkeg_time_diff\" : [\n",
    "        \"bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg.csv\",\n",
    "        \"bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_2_filtered.csv\",\n",
    "        \"bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_3_filtered.csv\",\n",
    "        \"bosf/all_temporal_extended_20_dates_644_points_381bosf_it_0.csv\",\n",
    "        \"bosf/all_temporal_extended_20_dates_644_points_381bosf_it_1_filtered.csv\",\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_precip_sum[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "temp_explanatory_dfs\n",
    "\n",
    "print(len(temp_explanatory_dfs))\n",
    "\n",
    "temp_explanatory_dfs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_file_names = [\n",
    "    \"data/7_training_data/Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"data/7_training_data/Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"data/7_training_data/Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"data/7_training_data/Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"data/7_training_data/Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"data/7_training_data/Hansen_year_2023_points_2075_f.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2016_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2017_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2018_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2019_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2020_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2021_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2022_points_377_f_bosf.csv\",\n",
    "    \"data/7_training_data/bosf/Hansen_year_2023_points_377_f_bosf.csv\",\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)\n",
    "\n",
    "hansen_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/bosf/\")\n",
    "\n",
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_file_name = \"All_Non_temporal_points_408_bosf_and_all.csv\"\n",
    "\n",
    "# for bosf\n",
    "# non_temporal_file_name = \"All_Non_temporal_points_381_bosf.csv\"\n",
    "\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)\n",
    "# drop lat and lon if they exist\n",
    "\n",
    "columns_to_drop = [\"lat\", \"lon\", \"date\", \"time_difference\"]\n",
    "non_temporal_df = non_temporal_df.drop(columns=[col for col in columns_to_drop if col in non_temporal_df.columns])\n",
    "# non_temporal_df\n",
    "\n",
    "print(non_temporal_df.columns)\n",
    "print(len(non_temporal_df.id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)\n",
    "explanatory_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.parameters import explain_vars\n",
    "export_vars = explain_vars + ['id', 'date', \"time_difference\"]\n",
    "# Remove \"doy\" because we have not created it yet\n",
    "\n",
    "export_vars.remove(\"doy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Final step: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var = df[[\"source\", \"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(\n",
    "    explanatory_df[export_vars], on=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "\n",
    "print(\"lenght of base_df: \", len(explanatory_with_response_var))\n",
    "# Drop duplicates on id and date\n",
    "explanatory_with_response_var = explanatory_with_response_var.drop_duplicates(subset=[\"id\", \"date\"])\n",
    "print(\"lenght of explanatory_with_response_var after dropping duplicates: \", len(explanatory_with_response_var))\n",
    "\n",
    "# Fill time_difference with 0 if is nan\n",
    "explanatory_with_response_var[\"time_difference\"] = explanatory_with_response_var[\"time_difference\"].fillna(0)\n",
    "\n",
    "explanatory_with_response_var.to_csv(\"data/7_training_data/bosf/explanatory_with_response_var_and_source_extra_bosf_brgm_all_extras.csv\", index=False)\n",
    "len(explanatory_with_response_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the data\n",
    "explanatory_with_response_var.to_csv(\"data/7_training_data/bosf/explanatory_with_response_var_and_source_extra_sum_prec_brgm_exact_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Optional (find missing points on second closest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_missing_points(df, explanatory_with_response_var):\n",
    "    \"\"\"This is an extra step only for bosf.\n",
    "\n",
    "        # We have gotten the data for the first closest date, this will leave some \n",
    "        # points without data (see notes.rst for more info), we need to get the data for the second closest date, but first we need to find the points that are missing data. \n",
    "        # We do that by just creating an unique id with the date and the id of the point and then we filter the points that are not in the explanatory_with_response_var dataframe.\n",
    "\n",
    "        # Get the missing points\"\"\"\n",
    "\n",
    "    df[\"date_id\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \"_\" + df[\"id\"].astype(str)\n",
    "    print(df.date_id.nunique())\n",
    "    explanatory_with_response_var[\"date_id\"] = explanatory_with_response_var[\"date\"].dt.strftime(\"%Y-%m-%d\") + \"_\" + explanatory_with_response_var[\"id\"].astype(str)\n",
    "\n",
    "    missing_points = df[\n",
    "        (~df[\"date_id\"].isin(explanatory_with_response_var[\"date_id\"]))\n",
    "    ]\n",
    "    missing_points = missing_points.drop_duplicates(subset=[\"id\", \"date\"])\n",
    "\n",
    "    print(\"Missing points: \", len(missing_points))\n",
    "\n",
    "    print(len(df)- len(missing_points) )\n",
    "\n",
    "    # Export the missing points \n",
    "    return missing_points\n",
    "\n",
    "\n",
    "missing_points = get_missing_points(df, explanatory_with_response_var)\n",
    "missing_points.to_csv(\"data/7_training_data/bosf/missing_points_brgm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I already requested all the data for the second closest date, I can just filter the points by\n",
    "# The previous result.\n",
    "\n",
    "# read second closest date data\n",
    "\n",
    "all_temp_2nd_closest = pd.read_csv(\"data/7_training_data/bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_2.csv\")\n",
    "\n",
    "# Create the unique id with the date (YYYY-MM-DD) and the id of the point\n",
    "all_temp_2nd_closest[\"date_id\"] = all_temp_2nd_closest[\"date\"].str.split(\"T\").str[0] + \"_\" + all_temp_2nd_closest[\"id\"].astype(str)\n",
    "\n",
    "# Filter out the points that are not in the all_temp_2nd_closest dataframe\n",
    "all_temp_missing_2nd = all_temp_2nd_closest[all_temp_2nd_closest[\"date_id\"].isin(missing_points[\"date_id\"])]\n",
    "\n",
    "# Export the missing temporal data\n",
    "print(len(all_temp_missing_2nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_temp_missing_2nd.to_csv(\"data/7_training_data/bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_2_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the third closest date\n",
    "\n",
    "all_temp_3rd_closest = pd.read_csv(\"data/7_training_data/bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_3.csv\")\n",
    "\n",
    "# Create the unique id with the date (YYYY-MM-DD) and the id of the point\n",
    "all_temp_3rd_closest[\"date_id\"] = all_temp_3rd_closest[\"date\"].str.split(\"T\").str[0] + \"_\" + all_temp_3rd_closest[\"id\"].astype(str)\n",
    "\n",
    "# Filter out the points that are not in the all_temp_3rd_closest dataframe and and not in the 2nd closest\n",
    "all_temp_missing_3rd = all_temp_3rd_closest[all_temp_3rd_closest[\"date_id\"].isin(missing_points[\"date_id\"]) & (~all_temp_3rd_closest[\"date_id\"].isin(all_temp_missing_2nd[\"date_id\"]))]\n",
    "\n",
    "all_temp_missing_3rd[\"time_difference\"].describe()\n",
    "\n",
    "all_temp_missing_3rd.to_csv(\"data/7_training_data/bosf/all_temporal_extended_8_dates_1559_points_31brgm_pkeg_position_3_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwl-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
