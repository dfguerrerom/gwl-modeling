{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following notebook will get two different datasets of explanatory variables: temporal an non-temporal\n",
    "# related. \n",
    "# In order to improve the speed time, this notebook will create the respective datasets and it will send a task to \n",
    "# EarthEngine with a ReduceByRegion operation, we have proved that this method is faster than using the individual\n",
    "# calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import ee\n",
    "from gee_scripts.get_sources import get_s1_image, get_gldas, get_gpm, get_hansen\n",
    "from gee_scripts.get_sources import get_srtm, get_globcover, get_gedi, get_gldas_stats\n",
    "ee.Initialize(project=\"ee-dfgm2006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective would be to loop over the points or the dates...<br>\n",
    "After testing this script https://code.earthengine.google.com/b18e876cca44266be704924b7354ddff <br>\n",
    "I found out that the best way to do it is to loop over the dates, and then pass the reduceregions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/field_data_unique_coords.csv')\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "## Set type of output\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run entirely, if we want to task the orders to GEE we'll set this variable to True\n",
    "send_task = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coords = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "unique_coords.head()\n",
    "len(unique_coords)\n",
    "\n",
    "# Convert them as a geodataframe and save them\n",
    "geometry = [Point(xy) for xy in zip(unique_coords.lon, unique_coords.lat)]\n",
    "gdf = gpd.GeoDataFrame(unique_coords, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read regions shapefile\n",
    "\n",
    "# I have two regions, first is to only the high correlated stations and the second is all the stations\n",
    "# I will use either depending on the dataset we have selected above\n",
    "\n",
    "shp_path = Path(\"data/0_shp/\")\n",
    "region_path = \"regions_to_request_explanatory_all.gpkg\"\n",
    "\n",
    "gdf_regions = gpd.GeoDataFrame.from_file(shp_path/region_path)\n",
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those date where the gwl measure is out of reasonable range\n",
    "upper_thres = 20\n",
    "lower_thres = -100\n",
    "\n",
    "df = df[(df.gwl_cm < upper_thres) & (df.gwl_cm > lower_thres)]\n",
    "\n",
    "# Get the coordinates of the individual points\n",
    "\n",
    "unique_coords = df[[\"id\", 'lon', 'lat']].drop_duplicates()\n",
    "len(df), len(unique_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe from x y coordinates\n",
    "gdf_unique_coords = gpd.GeoDataFrame(unique_coords, geometry=gpd.points_from_xy(unique_coords.lon, unique_coords.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "# Add the region id to each point\n",
    "gdf_unique_coords = gpd.sjoin(gdf_unique_coords, gdf_regions[[\"region_id\", \"geometry\"]], how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failing datasets array(['BRG_150710_01', 'BRG_621107_03'], dtype=object)\n",
    "gdf_unique_coords[gdf_unique_coords.id.isin([\"BRG_621107_03\", \"BRG_150710_01\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get temporal explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_unique_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_selectors = [\"system:index\", \"lat\", \"lon\", \"id\", \"date\"]\n",
    "s1_selectors = [\"LIA\", \"VH\", \"VV\", \"VVVH_ratio\", \"angle\"]\n",
    "gldas_selectors = ['sm_1', 'sm_3', 'sm_7', 'sm_30']\n",
    "gpm_selectors = ['precipitation', 'prec_3', 'prec_7', 'prec_30']\n",
    "\n",
    "def get_temporal_explanatory(region_id):\n",
    "    \"\"\"Get the explanatory temporal based variables\"\"\"\n",
    "\n",
    "    region = gdf_regions[gdf_regions.region_id == region_id].to_crs(\"EPSG:4326\")[:]\n",
    "    dates = df[df.id.isin(gdf_unique_coords[gdf_unique_coords.region_id == region_id].id.unique())].date.unique()\n",
    "    points = gdf_unique_coords[gdf_unique_coords.region_id == region_id][[\"id\", \"geometry\", \"lat\", \"lon\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # print(len(dates), len(points))\n",
    "    # Convert to ee elements\n",
    "\n",
    "    ee_dates = ee.FeatureCollection(ee.List([ ee.Feature(None, {\"date\": date}) for date in dates]))\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "    ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "\n",
    "    def get_sources(date_feature):\n",
    "\n",
    "        date_range = ee.Date(date_feature.get(\"date\")).getRange('day')\n",
    "\n",
    "        s1_composite = get_s1_image(date_range, ee_region)\n",
    "\n",
    "        return s1_composite.set({\n",
    "         \"numberOfBands\" : s1_composite.bandNames().size(),\n",
    "         \"date\" : ee.Date(date_feature.get(\"date\"))\n",
    "         })\n",
    "\n",
    "    def reduce_composite(composite):\n",
    "\n",
    "        # Filter the extra data with the matching date\n",
    "        date = composite.get(\"date\")\n",
    "        date_range = ee.Date(date).getRange('day')\n",
    "\n",
    "        gldas_composite = get_gldas(date_range, ee_region)\n",
    "        gpm_composite = get_gpm(date_range, ee_region)\n",
    "\n",
    "        composite = (ee.Image(composite)\n",
    "            .addBands(gldas_composite)\n",
    "            .addBands(gpm_composite)\n",
    "        )\n",
    "\n",
    "        return composite.reduceRegions(**{\n",
    "         \"collection\" : ee_points,\n",
    "         \"reducer\" : ee.Reducer.first(),\n",
    "         \"scale\" : 10,\n",
    "         \"tileScale\" : 16\n",
    "        }).filter(ee.Filter.notNull(['VH'])).map(lambda feature: feature.set({\n",
    "         \"date\" : date\n",
    "        }))\n",
    "\n",
    "\n",
    "    task = (ee_dates\n",
    "         .map(get_sources)\n",
    "         .filter(ee.Filter.gt('numberOfBands', 0))\n",
    "         .map(reduce_composite).flatten()\n",
    "    )\n",
    "\n",
    "    task_name = f\"All_temporal_non_resample_at_all_region_{region_id}_dates_{len(dates)}_points_{len(points)}_with_date_lon_lat\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "      \"collection\": task, \n",
    "      \"folder\" : \"INDONESIA_GWL\",\n",
    "      \"description\": task_name,\n",
    "      \"selectors\": base_selectors + s1_selectors + gldas_selectors + gpm_selectors\n",
    "    })\n",
    "\n",
    "    # Uncoment to start the task\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n",
    "\n",
    "[get_temporal_explanatory(region_id) for region_id in gdf_regions.region_id.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Get \"yearly\" temporal explanatory variables (Hansen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansen_selectors = [\"year\", \"B3\",\"B4\",\"B5\",\"B7\",\"ndvi\",\"ndmi\",\"ndbri\"]\n",
    "\n",
    "# get all the years from the field data\n",
    "years = sorted([y for y in df.date.dt.year.unique() if y != 2013] )\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    points = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "    points = gpd.GeoDataFrame(points, geometry=gpd.points_from_xy(points.lon, points.lat), crs=\"EPSG:4326\")\n",
    "    ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "\n",
    "    image = get_hansen(year)\n",
    "\n",
    "    result = image.reduceRegions(**{\n",
    "        \"collection\" : ee_points,\n",
    "        \"reducer\" : ee.Reducer.first(),\n",
    "        \"scale\" : 30,\n",
    "        \"tileScale\" : 16\n",
    "    }).map(lambda feature: feature.set(\"year\", str(year)))\n",
    "    \n",
    "    task_name = f\"Hansen_year_{year}_points_{len(points)}_f\"\n",
    "\n",
    "    ee_task = ee.batch.Export.table.toDrive(**{\n",
    "        \"collection\": result, \n",
    "        \"folder\" : \"INDONESIA_GWL\",\n",
    "        \"description\": f\"Hansen_year_{year}_points_{len(points)}_f\",\n",
    "        \"selectors\": base_selectors + hansen_selectors\n",
    "    })\n",
    "\n",
    "    not send_task or ee_task.start()\n",
    "    print(\"Exported\" if send_task else \"Not exported\", task_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get non temporal explanatory variables (others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is not too computational expensive, so we are not forced to chunk it\n",
    "# We'll try to get all the points at once, not by region (so we won't filter by region)\n",
    "region = gdf_regions.to_crs(\"EPSG:4326\")[:]\n",
    "ee_region = ee.FeatureCollection(region.__geo_interface__)\n",
    "points = gdf_unique_coords[[\"id\", \"geometry\", \"lat\", \"lon\"]].rename(columns={\"id\": \"id\"}).to_crs(\"EPSG:4326\")\n",
    "ee_points = ee.FeatureCollection(points.__geo_interface__)\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (\n",
    "    get_srtm()\n",
    "        .addBands(get_globcover())\n",
    "        .addBands(get_gedi(ee_region))\n",
    "        .addBands(get_gldas_stats(ee_region))\n",
    ")\n",
    "composite.bandNames().getInfo()\n",
    "\n",
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['canopy_height']))\n",
    "\n",
    "task_name = f\"All_Non_temporal_points_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['elevation', 'aspect', 'slope', 'land_cov', 'canopy_height', \"gldas_mean\", \"gldas_stddev\"]\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Extra Non temporal explanatory variables (others)\n",
    "\n",
    "This data comes from https://code.earthengine.google.com/6c3eeb929a5ee8a42f55234b58796c0a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phu = ee.FeatureCollection(\"users/marortpab/FAO/SEPAL/2023_trainings/smm/AOI__Province__865_PHUs__INDONESIA\")\n",
    "water = ee.FeatureCollection(\"users/marortpab/FAO/SEPAL/2023_trainings/smm/water_bodies_phu_buff_1_km_def\")\n",
    "canals = ee.FeatureCollection(\"users/marortpab/FAO/SEPAL/2023_trainings/smm/prims_canal_data\")\n",
    "\n",
    "hydro_dir = ee.Image('WWF/HydroSHEDS/03DIR')\n",
    "drain_direction = hydro_dir.select('b1').rename('dir')\n",
    "hydro_acc = ee.Image('WWF/HydroSHEDS/15ACC')\n",
    "flow_acc = hydro_acc.select('b1').rename('acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //Merge water bodies and canals\n",
    "\n",
    "all_water = water.merge(canals)\n",
    "distance = all_water.distance(25000)\n",
    "\n",
    "composite = (\n",
    "    distance\n",
    "        .addBands(drain_direction)\n",
    "        .addBands(flow_acc)\n",
    ")\n",
    "composite.bandNames().getInfo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = composite.reduceRegions(**{\n",
    "    \"collection\" : ee_points,\n",
    "    \"reducer\" : ee.Reducer.first(),\n",
    "    \"scale\" : 10,\n",
    "    \"tileScale\" : 16\n",
    "}).filter(ee.Filter.notNull(['distance']))\n",
    "\n",
    "task_name = f\"All_Non_temporal_extra_points_latlon_{len(points)}\"\n",
    "\n",
    "ee_task = ee.batch.Export.table.toDrive(**{\n",
    "    \"collection\": result, \n",
    "    \"folder\" : \"INDONESIA_GWL\",\n",
    "    \"description\":task_name,\n",
    "    \"selectors\": base_selectors + ['distance', 'dir', 'acc']\n",
    "})\n",
    "\n",
    "# Uncoment to start the task\n",
    "not send_task or ee_task.start()\n",
    "print(\"Exported\" if send_task else \"Not exported\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Read temporal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "dataset = \"all\"\n",
    "temporal_file_names_groups = {\n",
    "    \"all\" : [\n",
    "        \"All_temporal_non_resample_at_all_region_1_dates_520_points_24_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_2_dates_1773_points_148_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_3_dates_479_points_1_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_4_dates_988_points_348_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_5_dates_1796_points_717_with_date.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_6_dates_489_points_43_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_7_dates_1274_points_477_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_8_dates_1671_points_220_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_9_dates_379_points_17_with_date_lon_lat.csv\",\n",
    "        \"All_temporal_non_resample_at_all_region_10_dates_846_points_77_with_date_lon_lat.csv\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is the name of the type of data we're using (high_corr or all) (it's assigned at the beginning of the notebook)\n",
    "temporal_file_names = temporal_file_names_groups[dataset]\n",
    "\n",
    "# get and concatenate all the dataframes\n",
    "temp_explanatory_dfs = pd.concat([\n",
    "            pd.read_csv(explanatory_path/file_name) \n",
    "            for file_name \n",
    "            in temporal_file_names\n",
    "        ], \n",
    ")\n",
    "\n",
    "temp_explanatory_dfs[\"date\"] = pd.to_datetime(temp_explanatory_dfs[\"date\"])\n",
    "temp_explanatory_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate id and date\n",
    "temp_explanatory_dfs.drop_duplicates([\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Read Hansen yearly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_path = Path(\"data/7_training_data/\")\n",
    "hansen_file_names = [\n",
    "    \"Hansen_year_2018_points_2075_f.csv\",\n",
    "    \"Hansen_year_2019_points_2075_f.csv\",\n",
    "    \"Hansen_year_2020_points_2075_f.csv\",\n",
    "    \"Hansen_year_2021_points_2075_f.csv\",\n",
    "    \"Hansen_year_2022_points_2075_f.csv\",\n",
    "    \"Hansen_year_2023_points_2075_f.csv\"\n",
    "]\n",
    "\n",
    "hansen_df = pd.concat([\n",
    "    pd.read_csv(explanatory_path/file_name) \n",
    "    for file_name \n",
    "    in hansen_file_names\n",
    "], axis=0)\n",
    "hansen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Read non temporal explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_file_name = \"All_Non_temporal_points_2074.csv\"\n",
    "non_temporal_df = pd.read_csv(explanatory_path/non_temporal_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_df = non_temporal_df.drop(columns=[\"lat\", \"lon\"])\n",
    "non_temporal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Read extra non temporal explanatory (accumulation, distance to rivers/canals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the non-temporal variables are the same for all the points, we just need to duplicate \n",
    "# their results into each of the dates of the points.\n",
    "# i.e. 1 point with 10 dates will have the same non-temporal variables for each of the 10 dates.\n",
    "\n",
    "non_temporal_extra_file_name = \"All_Non_temporal_extra_points_latlon_2072.csv\"\n",
    "non_temporal_extra_df = pd.read_csv(explanatory_path/non_temporal_extra_file_name)\n",
    "# drop lat and lon\n",
    "non_temporal_extra_df = non_temporal_extra_df.drop(columns=[\"lat\", \"lon\", \"date\"])\n",
    "non_temporal_extra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create final explanatory variables dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_explanatory_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non-temporal variables with the temporal ones\n",
    "explanatory_df = temp_explanatory_dfs.merge(non_temporal_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the extra non-temporal variables\n",
    "explanatory_df = explanatory_df.merge(non_temporal_extra_df, on=\"id\")\n",
    "len(explanatory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hansen data with year and id\n",
    "explanatory_df[\"year\"] = explanatory_df.date.dt.year\n",
    "hansen_df[\"year\"] = hansen_df[\"year\"].astype(int)\n",
    "explanatory_df = explanatory_df.merge(hansen_df[[\"id\"] + hansen_selectors], on=[\"id\", \"year\"], how=\"left\")\n",
    "\n",
    "# I get more values here because I have requested Hansen for all the years\n",
    "explanatory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vars = [\n",
    "    'id', 'date', 'LIA', 'VH', 'VV', 'VVVH_ratio',\n",
    "    'angle', 'sm_1', 'sm_3', 'sm_7', 'sm_30', 'precipitation', 'prec_3',\n",
    "    'prec_7', 'prec_30', 'elevation',\n",
    "    'aspect', 'slope', 'land_cov', 'canopy_height', 'gldas_mean',\n",
    "    'gldas_stddev', 'B3', 'B4',\n",
    "    'B5', 'B7', 'ndvi', 'ndmi', 'ndbri',\n",
    "    'distance', 'dir', 'acc'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Final step: Merge explanatory variables with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_with_response_var = df[[\"source\", \"id\", \"date\", \"gwl_cm\", \"lat\", \"lon\"]].merge(explanatory_df[export_vars], on=[\"id\", \"date\"])\n",
    "\n",
    "# Add day of the year as a variable\n",
    "explanatory_with_response_var[\"doy\"] = explanatory_with_response_var.date.dt.dayofyear\n",
    "explanatory_with_response_var.to_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>gwl_cm</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>LIA</th>\n",
       "      <th>VH</th>\n",
       "      <th>VV</th>\n",
       "      <th>VVVH_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B7</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>ndmi</th>\n",
       "      <th>ndbri</th>\n",
       "      <th>distance</th>\n",
       "      <th>dir</th>\n",
       "      <th>acc</th>\n",
       "      <th>doy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pkeg</td>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>32.785855</td>\n",
       "      <td>-11.481278</td>\n",
       "      <td>-5.556430</td>\n",
       "      <td>0.207099</td>\n",
       "      <td>...</td>\n",
       "      <td>97</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>4519.468722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pkeg</td>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2021-08-24</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>32.785295</td>\n",
       "      <td>-12.812067</td>\n",
       "      <td>-5.960235</td>\n",
       "      <td>0.201164</td>\n",
       "      <td>...</td>\n",
       "      <td>97</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>4519.468722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pkeg</td>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2022-04-09</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>32.795191</td>\n",
       "      <td>-18.437775</td>\n",
       "      <td>-10.636812</td>\n",
       "      <td>0.072032</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>52</td>\n",
       "      <td>0.359223</td>\n",
       "      <td>-0.156627</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>4519.468722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pkeg</td>\n",
       "      <td>02_AHL_SBG-B076</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>3.937760</td>\n",
       "      <td>117.007750</td>\n",
       "      <td>32.793740</td>\n",
       "      <td>-13.051827</td>\n",
       "      <td>-9.007584</td>\n",
       "      <td>0.076149</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>52</td>\n",
       "      <td>0.359223</td>\n",
       "      <td>-0.156627</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>4519.468722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkeg</td>\n",
       "      <td>02_AHL_SBG-B101</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>3.931860</td>\n",
       "      <td>117.010120</td>\n",
       "      <td>39.267563</td>\n",
       "      <td>-9.778736</td>\n",
       "      <td>-6.648683</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5020.676546</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33415</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kalteng1</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>-23.9</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>32.478706</td>\n",
       "      <td>-12.878587</td>\n",
       "      <td>-8.271499</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>189.692997</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kalteng1</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>-31.1</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>32.478589</td>\n",
       "      <td>-13.078360</td>\n",
       "      <td>-6.593164</td>\n",
       "      <td>0.169898</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>189.692997</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33417</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kalteng1</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>-14.7</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>32.479968</td>\n",
       "      <td>-12.563061</td>\n",
       "      <td>-7.793232</td>\n",
       "      <td>0.110794</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>189.692997</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33418</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kalteng1</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>-27.1</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>32.480674</td>\n",
       "      <td>-13.179518</td>\n",
       "      <td>-7.523130</td>\n",
       "      <td>0.128794</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>189.692997</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33419</th>\n",
       "      <td>old_brg</td>\n",
       "      <td>kalteng1</td>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>-2.319728</td>\n",
       "      <td>114.058131</td>\n",
       "      <td>32.474638</td>\n",
       "      <td>-12.679995</td>\n",
       "      <td>-7.348031</td>\n",
       "      <td>0.130210</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>189.692997</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33420 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source               id        date  gwl_cm       lat         lon  \\\n",
       "0         pkeg  02_AHL_SBG-B076  2021-06-01   -41.0  3.937760  117.007750   \n",
       "1         pkeg  02_AHL_SBG-B076  2021-08-24   -38.0  3.937760  117.007750   \n",
       "2         pkeg  02_AHL_SBG-B076  2022-04-09   -22.0  3.937760  117.007750   \n",
       "3         pkeg  02_AHL_SBG-B076  2023-02-15   -31.0  3.937760  117.007750   \n",
       "4         pkeg  02_AHL_SBG-B101  2021-06-01   -66.0  3.931860  117.010120   \n",
       "...        ...              ...         ...     ...       ...         ...   \n",
       "33415  old_brg         kalteng1  2020-02-24   -23.9 -2.319728  114.058131   \n",
       "33416  old_brg         kalteng1  2020-03-07   -31.1 -2.319728  114.058131   \n",
       "33417  old_brg         kalteng1  2020-03-19   -14.7 -2.319728  114.058131   \n",
       "33418  old_brg         kalteng1  2020-03-31   -27.1 -2.319728  114.058131   \n",
       "33419  old_brg         kalteng1  2020-04-12   -11.4 -2.319728  114.058131   \n",
       "\n",
       "             LIA         VH         VV  VVVH_ratio  ...  B4  B5  B7      ndvi  \\\n",
       "0      32.785855 -11.481278  -5.556430    0.207099  ...  97  61  25  0.644068   \n",
       "1      32.785295 -12.812067  -5.960235    0.201164  ...  97  61  25  0.644068   \n",
       "2      32.795191 -18.437775 -10.636812    0.072032  ...  70  96  52  0.359223   \n",
       "3      32.793740 -13.051827  -9.007584    0.076149  ...  70  96  52  0.359223   \n",
       "4      39.267563  -9.778736  -6.648683    0.111111  ...  75  57  25  0.562500   \n",
       "...          ...        ...        ...         ...  ...  ..  ..  ..       ...   \n",
       "33415  32.478706 -12.878587  -8.271499    0.097345  ...  81  53  21  0.620000   \n",
       "33416  32.478589 -13.078360  -6.593164    0.169898  ...  81  53  21  0.620000   \n",
       "33417  32.479968 -12.563061  -7.793232    0.110794  ...  81  53  21  0.620000   \n",
       "33418  32.480674 -13.179518  -7.523130    0.128794  ...  81  53  21  0.620000   \n",
       "33419  32.474638 -12.679995  -7.348031    0.130210  ...  81  53  21  0.620000   \n",
       "\n",
       "           ndmi     ndbri     distance  dir  acc  doy  \n",
       "0      0.227848  0.590164  4519.468722    1    1  152  \n",
       "1      0.227848  0.590164  4519.468722    1    1  236  \n",
       "2     -0.156627  0.147541  4519.468722    1    1   99  \n",
       "3     -0.156627  0.147541  4519.468722    1    1   46  \n",
       "4      0.136364  0.500000  5020.676546  128   36  152  \n",
       "...         ...       ...          ...  ...  ...  ...  \n",
       "33415  0.208955  0.588235   189.692997  128   16   55  \n",
       "33416  0.208955  0.588235   189.692997  128   16   67  \n",
       "33417  0.208955  0.588235   189.692997  128   16   79  \n",
       "33418  0.208955  0.588235   189.692997  128   16   91  \n",
       "33419  0.208955  0.588235   189.692997  128   16  103  \n",
       "\n",
       "[33420 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"data/7_training_data/explanatory_with_response_var_and_source_extra.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(test) gwl-modeling",
   "language": "python",
   "name": "gwl-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
