{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore data sources from .csv files, merge them and save them\n",
    "brg_path = Path(\"data/2_brg/brg_raw.csv\")\n",
    "pkeg_path = Path(\"data/3_pkeg/pkeg_raw.csv\")\n",
    "old_brg_path = Path(\"data/4_brg_old/brg_old.csv\")\n",
    "new_bosf_path = Path(\"data/1_bosf_data/bosf_data_csv.csv\")\n",
    "\n",
    "DATA_COLS = [\"source\", \"id\", \"lon\", \"lat\", \"date\", \"gwl_cm\"]\n",
    "\n",
    "# Check all files exist\n",
    "for path in [brg_path, pkeg_path, old_brg_path, new_bosf_path]:\n",
    "    assert path.exists(), f\"File not found: {path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRGM new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brg_df = pd.read_csv(brg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brg_cols_rename = {\n",
    "    \"gwl_rata\" : \"gwl_cm\",\n",
    "}\n",
    "\n",
    "# rename columns\n",
    "brg_df.rename(columns=brg_cols_rename, inplace=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "brg_df[\"date\"] = pd.to_datetime(brg_df[\"date\"])\n",
    "\n",
    "# multiply gwl_cm by 100 to convert it to cm\n",
    "brg_df[\"gwl_cm\"] = brg_df[\"gwl_cm\"] * 100\n",
    "\n",
    "# set a new column for source\n",
    "brg_df[\"source\"] = \"brg\"\n",
    "\n",
    "brg_df = brg_df[DATA_COLS]\n",
    "brg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brg_df.id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pkeg's data\n",
    "pkeg_df = pd.read_csv(pkeg_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkeg_cols_rename = {\n",
    "    \"date(dd/mm/yyyy)\" : \"date\",\n",
    "    \"gwl(cm)\" : \"gwl_cm\",\n",
    "    \"coor_y(dd)\" : \"lat\",\n",
    "    \"coor_x(dd)\" : \"lon\",\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "pkeg_df.rename(columns=pkeg_cols_rename, inplace=True)\n",
    "\n",
    "# combine \"kode_perusahaan\"\tand \"kode_titik\" to create a unique id\n",
    "pkeg_df.loc[:, \"id\"] = pkeg_df[\"kode_perusahaan\"] + \"_\" + pkeg_df[\"kode_titik\"]\n",
    "\n",
    "# Convert date column to datetime\n",
    "pkeg_df.loc[:, \"date\"] = pd.to_datetime(pkeg_df[\"date\"], dayfirst=True)\n",
    "\n",
    "# set a new column for source\n",
    "pkeg_df[\"source\"] = \"pkeg\"\n",
    "\n",
    "# Only select columns that are needed\n",
    "pkeg_df = pkeg_df[DATA_COLS]\n",
    "pkeg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous BRG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_brg_df = pd.read_csv(old_brg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_brg_df.loc[:, \"source\"] = \"old_brg\"\n",
    "old_brg_df.loc[:, \"date\"] = pd.to_datetime(old_brg_df[\"date\"], dayfirst=True)\n",
    "\n",
    "# Multiply gwl_cm by 100 to convert it to cm\n",
    "old_brg_df[\"gwl_cm\"] = old_brg_df[\"gwl_cm\"] * 100\n",
    "old_brg_df = old_brg_df[DATA_COLS]\n",
    "old_brg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New bosf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bosf_df = pd.read_csv(new_bosf_path)\n",
    "\n",
    "# Create a single id by concat some columns\n",
    "\n",
    "new_bosf_df.loc[:, \"id\"] = new_bosf_df[\"Project ID\"].astype(str) + new_bosf_df[\"Site Cd\"].astype(str) + new_bosf_df[\"Point Ref\"].astype(str) + new_bosf_df[\"Point Index\"].astype(str) + new_bosf_df[\"Instrument ID\"].astype(str)\n",
    "\n",
    "new_bosf_cols_rename = {\n",
    "    \"Date field\" : \"date\",\n",
    "    \"Water table depth (WTD) (cm)\" : \"gwl_cm\",\n",
    "    \"Latitude\" : \"lat\",\n",
    "    \"Longitude\" : \"lon\",\n",
    "    \"Project ID\" : \"source\"\n",
    "}\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "new_bosf_df.rename(columns=new_bosf_cols_rename, inplace=True)\n",
    "\n",
    "# give a new source name\n",
    "new_bosf_df.loc[:, \"source\"] = \"bosf_\" + new_bosf_df[\"source\"]\n",
    "\n",
    "# parse date column\n",
    "new_bosf_df.loc[:, \"date\"] = pd.to_datetime(new_bosf_df[\"date\"])\n",
    "\n",
    "new_bosf_df = new_bosf_df[DATA_COLS]\n",
    "new_bosf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df = pd.concat([brg_df, pkeg_df, old_brg_df, new_bosf_df], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as csv\n",
    "df.to_csv(\"data/field_data_all_with_old_plus_bosf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate dates for each id using the mean value\n",
    "# Group by 'id' and 'date' and calculate the mean value for 'gwl_cm' while keeping other columns\n",
    "\n",
    "print(\"Before removing duplicates\", len(df))\n",
    "\n",
    "agg_dict = {'source':'first','lon':'first','lat':'first','gwl_cm':'mean'}\n",
    "df = df.groupby(['id','date']).agg(agg_dict).reset_index()\n",
    "\n",
    "print(\"After removing duplicates\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of new bosf data\n",
    "bosf_count = df[df[\"source\"].str.contains(\"bosf\")].shape[0]\n",
    "print(f\"Number of new bosf data: {bosf_count}\")\n",
    "\n",
    "# print the number before and after removing duplicates\n",
    "print(\"Number of data before removing duplicates: \", len(new_bosf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I will remove the duplicated coordinates IDS and keep the first one\n",
    "# get unique lon-lat pairs\n",
    "unique = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "# Get duplicated lon-lat pairs\n",
    "duplicated = unique[unique.duplicated(subset=[\"lon\", \"lat\"], keep=False)]\n",
    "\n",
    "duplicated = duplicated.drop_duplicates(subset=[\"lon\", \"lat\"], keep=\"first\")\n",
    "\n",
    "# Get the duplicated ids\n",
    "duplicated_ids = duplicated[\"id\"].unique()\n",
    "\n",
    "# # get dataframe without duplicated ids\n",
    "\n",
    "df = df[~df[\"id\"].isin(duplicated_ids)]\n",
    "\n",
    "df.to_csv(\"data/field_data_unique_coords_plus_bosf.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique coordinates for each station\n",
    "stations = df[[\"id\", \"source\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(stations.lon, stations.lat)]\n",
    "stations_gdf = GeoDataFrame(stations, geometry=geometry)\n",
    "stations_gdf.crs = \"EPSG:4326\"\n",
    "stations_gdf.to_file(\"data/0_shp/unique_stations_no_repeated_plus_bosf.gpkg\", driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
