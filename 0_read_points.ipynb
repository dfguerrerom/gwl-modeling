{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore data sources from .xslsx files, merge them and save them as .csv\n",
    "\n",
    "ach_path = Path(\"data/2_Achmad/achmed_raw.csv\")\n",
    "wal_path = Path(\"data/3_Waluyo/waluyo_raw.csv\")\n",
    "old_brg_path = Path(\"data/4_brg_old/brg_old.csv\")\n",
    "\n",
    "DATA_COLS = [\"source\", \"id\", \"lon\", \"lat\", \"date\", \"gwl_cm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ach_df = pd.read_csv(ach_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRGM new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ach_cols_rename = {\n",
    "    \"gwl_rata\" : \"gwl_cm\",\n",
    "}\n",
    "\n",
    "# rename columns\n",
    "ach_df.rename(columns=ach_cols_rename, inplace=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "ach_df[\"date\"] = pd.to_datetime(ach_df[\"date\"])\n",
    "\n",
    "# multiply gwl_cm by 100 to convert it to cm\n",
    "ach_df[\"gwl_cm\"] = ach_df[\"gwl_cm\"] * 100\n",
    "\n",
    "# set a new column for source\n",
    "ach_df[\"source\"] = \"ach\"\n",
    "\n",
    "ach_df = ach_df[DATA_COLS]\n",
    "ach_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ach_df.id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wal_cols_rename = {\n",
    "    \"date(dd/mm/yyyy)\" : \"date\",\n",
    "    \"gwl(cm)\" : \"gwl_cm\",\n",
    "    \"coor_y(dd)\" : \"lat\",\n",
    "    \"coor_x(dd)\" : \"lon\",\n",
    "}\n",
    "\n",
    "# Read Waluyo's data\n",
    "wal_df = pd.read_csv(wal_path, sep=\";\")\n",
    "\n",
    "# Rename columns\n",
    "wal_df.rename(columns=wal_cols_rename, inplace=True)\n",
    "\n",
    "# combine \"kode_perusahaan\"\tand \"kode_titik\" to create a unique id\n",
    "wal_df.loc[:, \"id\"] = wal_df[\"kode_perusahaan\"] + \"_\" + wal_df[\"kode_titik\"]\n",
    "\n",
    "# Convert date column to datetime\n",
    "wal_df.loc[:, \"date\"] = pd.to_datetime(wal_df[\"date\"], dayfirst=True)\n",
    "\n",
    "# set a new column for source\n",
    "wal_df[\"source\"] = \"wal\"\n",
    "\n",
    "# Only select columns that are needed\n",
    "wal_df = wal_df[DATA_COLS]\n",
    "wal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous BRG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_brg_df = pd.read_csv(old_brg_path)\n",
    "old_brg_df.loc[:, \"source\"] = \"old_brg\"\n",
    "old_brg_df.loc[:, \"date\"] = pd.to_datetime(old_brg_df[\"date\"], dayfirst=True)\n",
    "old_brg_df = old_brg_df[DATA_COLS]\n",
    "old_brg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_brg_df.id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df = pd.concat([ach_df, wal_df, old_brg_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# save the dataframe as csv\n",
    "df.to_csv(\"data/field_data_all_with_old.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate dates for each id using the mean value\n",
    "# Group by 'id' and 'date' and calculate the mean value for 'gwl_cm' while keeping other columns\n",
    "\n",
    "print(\"Before removing duplicates\", len(df))\n",
    "\n",
    "agg_dict = {'source':'first','lon':'first','lat':'first','gwl_cm':'mean'}\n",
    "df = df.groupby(['id','date']).agg(agg_dict).reset_index()\n",
    "\n",
    "print(\"After removing duplicates\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I will remove the duplicated coordinates IDS and keep the first one\n",
    "# get unique lon-lat pairs\n",
    "unique = df[[\"id\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "# Get duplicated lon-lat pairs\n",
    "duplicated = unique[unique.duplicated(subset=[\"lon\", \"lat\"], keep=False)]\n",
    "\n",
    "duplicated = duplicated.drop_duplicates(subset=[\"lon\", \"lat\"], keep=\"first\")\n",
    "\n",
    "# Get the duplicated ids\n",
    "duplicated_ids = duplicated[\"id\"].unique()\n",
    "\n",
    "# # get dataframe without duplicated ids\n",
    "\n",
    "df = df[~df[\"id\"].isin(duplicated_ids)]\n",
    "\n",
    "df.to_csv(\"data/field_data_unique_coords_2.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique coordinates for each station\n",
    "stations = df[[\"id\", \"source\", \"lon\", \"lat\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(stations.lon, stations.lat)]\n",
    "stations_gdf = GeoDataFrame(stations, geometry=geometry)\n",
    "stations_gdf.crs = \"EPSG:4326\"\n",
    "stations_gdf.to_file(\"data/0_shp/unique_stations_no_repeated.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unique points (and check they have the same coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique ids\n",
    "ids = df[\"id\"].unique()\n",
    "print(\"ids len\", len(ids))\n",
    "\n",
    "# Check that we have the same number of unique coordinates\n",
    "\n",
    "# get unique coordinates\n",
    "coords = df[[\"lon\", \"lat\"]].drop_duplicates()\n",
    "print(\"coords len\", len(coords))\n",
    "\n",
    "# Check the stations that have more than one coordinate\n",
    "\n",
    "# get the number of coordinates per id\n",
    "grouped = df.groupby(['lon', 'lat']).agg({'id': pd.Series.nunique}).reset_index()\n",
    "\n",
    "# Filter groups with more than one unique 'id'\n",
    "shared_coords = grouped[grouped['id'] > 1].reset_index()\n",
    "\n",
    "# For each shared coordinate, list the unique station IDs\n",
    "shared_ids = []\n",
    "for _, row in shared_coords.iterrows():\n",
    "    lon, lat = row['lon'], row['lat']\n",
    "    stations_at_coord = df[(df['lon'] == lon) & (df['lat'] == lat)]['id'].unique()\n",
    "    shared_ids.append(stations_at_coord.tolist())\n",
    "\n",
    "# flatten the list\n",
    "shared_ids = set([item for sublist in shared_ids for item in sublist])\n",
    "\n",
    "# Print the results\n",
    "# we'd say that \"half\" of the following have shared coordinates\n",
    "len(pd.DataFrame(shared_ids).iloc[:,0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert unique ids to shapefile \n",
    "\n",
    "Note that unique ids contains duplicated coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export unique point ID's as shapefile\n",
    "\n",
    "df_only_locations = df.drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "# Transfor date to string to avoid errors when exporting to shapefile\n",
    "df_only_locations[\"date\"] = df_only_locations[\"date\"].astype(str)\n",
    "\n",
    "df_only_locations = gpd.GeoDataFrame(df_only_locations, geometry=gpd.points_from_xy(df_only_locations[\"lon\"], df_only_locations[\"lat\"]))\n",
    "df_only_locations.crs = \"EPSG:4326\"\n",
    "df_only_locations.to_file(\"data/merged_df.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### (test()) Create a random sample of 3 points to extract their SM from the images\n",
    "\n",
    "# Create a random sample of 3 points to extract their SM from the images\n",
    "test_sample = gpd.GeoDataFrame(df_only_locations.sample(3, random_state=42))\n",
    "\n",
    "# Export the sample as shapefile\n",
    "test_sample.to_file(\"data/0_shp/test_sample.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "import datetime\n",
    "\n",
    "smm_df = pd.read_csv(\"data/6_extracted_sm_data/all_extracted_data.csv\")\n",
    "\n",
    "# convert date to datetime\n",
    "smm_df[\"date\"] = pd.to_datetime(smm_df[\"date\"])\n",
    "smm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join smm_df with data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes based on their point_id and date\n",
    "# If the point_id and date are the same, the SM value will be added to the df\n",
    "# If the point_id and date are not the same, the SM value will be NaN\n",
    "\n",
    "df_with_sm_data = df.merge(smm_df, how=\"left\", left_on=[\"id\", \"date\"], right_on=[\"point_id\", \"date\"])\n",
    "df_with_sm_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns that are needed\n",
    "\n",
    "cols_to_export = [\n",
    "    \"source\",\n",
    "    \"id\",\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"date\",\n",
    "    \"gwl_cm\",\n",
    "    \"smm_value\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_sm_data[cols_to_export].to_csv(\"data/field_data_with_sm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find stations with the same coordinate pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find stations with the same coordinate pair\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(test) gwl-modeling",
   "language": "python",
   "name": "gwl-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
