{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from gee_scripts.models import get_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read phu regions shapefile\n",
    "#phu_regions = gpd.read_file(\"data/0_shp/AOI__Province__865_PHUs__INDONESIA.gpkg\")\n",
    "#phu_regions = phu_regions.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/9_clean_training_data/clean_training_data.csv\")\n",
    "df.head()\n",
    "# Convert to GeoDataFrame\n",
    "#df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\")\n",
    "# do spatial join with phu's\n",
    "#df = gpd.sjoin(df, phu_regions, how=\"left\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of cases per PHU\n",
    "phu_cases = df.groupby(\"phu_id\").size().reset_index(name=\"observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a boxplot of response var per island but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"island\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"PHU id\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by Island\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a boxplot of response var per source but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"source\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"source\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a boxplot of response var per province but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "sns.boxplot(x=\"province\", y=\"gwl_cm\", data=df, width=0.5)\n",
    "\n",
    "# Rename x-axis with phu id\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"PHU id\")\n",
    "plt.ylabel(\"Groundwater Level (cm)\")\n",
    "plt.title(\"Groundwater Level Distribution by Province\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a boxplot showing the number of dates per each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group by id and get the number of dates for each id\n",
    "group_by = \"id\"\n",
    "df_grouped = df.groupby(group_by).count().reset_index()\n",
    "df_grouped = df_grouped[[group_by, \"date\"]]\n",
    "df_grouped.columns = [\"name\", \"date_count\"]\n",
    "df_grouped.sort_values(by=\"date_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dimensions of the plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a violin plot for the variable\n",
    "sns.violinplot(x=df_grouped[\"date_count\"])\n",
    "\n",
    "# Set the title and x-axis label\n",
    "plt.title(f\"Frequency dates per point\")\n",
    "plt.xlabel(\"Number of dates per station\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop all stations with less tan 9 observations\n",
    "min_obs = 9\n",
    "df.groupby('id').agg({'date': 'count'}).sort_values(by='date', ascending=False).reset_index()\n",
    "df = df.groupby('id').filter(lambda group: len(group) >= min_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gee_scripts.parameters import explain_vars, response_var\n",
    "print(\"dependent var\", response_var)\n",
    "print(\"explanatory lenght\", len(explain_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All but one test over stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gee_scripts.randomforest import run_randomforest\n",
    "from gee_scripts.randomforest import get_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_ids = pd.read_csv(\"data/high_corr_0.2_temporal_variables_station_ids.csv\")\n",
    "high_corr_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variable = 'gwl_cm'\n",
    "\n",
    "high_corr_ids = pd.read_csv(\"data/high_corr_0.2_temporal_variables_station_ids.csv\")\n",
    "high_corr_ids.columns = [\"id\"]\n",
    "\n",
    "training_df = df[\n",
    "  #  (df.island == \"Kalimantan\") & don't use the selection by island, the selected stations come from the previous notebook\n",
    "    df.id.isin(high_corr_ids.id.unique())\n",
    "]\n",
    "\n",
    "# Manually selected PHU for training\n",
    "# high_corr_phu_ids = [\n",
    "#     136,\n",
    "#     137,\n",
    "#     138,\n",
    "#     143\n",
    "# ]\n",
    "# training_df = df[\n",
    "#     (df.phu_id.isin(high_corr_phu_ids))\n",
    "# ]\n",
    "\n",
    "stats_df = run_randomforest(training_df, type_=\"allbutone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"r_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"rmse_local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_stations = stats_df[stats_df.rmse_local < 15].sort_values(by=\"r_local\", ascending=False).index\n",
    "best_stations\n",
    "len(best_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with best stations over all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from gee_scripts.parameters import explain_vars, temporal_expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split gdf into good statoins for train\n",
    "gdf_high = training_df[training_df.id.isin(best_stations)].copy()\n",
    "# and bad stations for test\n",
    "gdf_low = training_df[~training_df.id.isin(best_stations)].copy()\n",
    "\n",
    "variable = 'gwl_cm'\n",
    "\n",
    "# create and train classifier\n",
    "regr = get_regressor()\n",
    "regr.fit(gdf_high[explain_vars], gdf_high[variable])\n",
    "\n",
    "row = {}\n",
    "#rmse_list = []\n",
    "for station in gdf_low.id.unique():\n",
    "    explans = []\n",
    "    # apply model to specific station\n",
    "    gdf_test = gdf_low[gdf_low.id == station]\n",
    "    y_pred_test = regr.predict(gdf_test[explain_vars])\n",
    "\n",
    "    # get pearsons r\n",
    "    r, p = pearsonr(gdf_test[variable].values, y_pred_test)\n",
    "    explans.append(r)\n",
    "\n",
    "    explans.append(np.sqrt(mean_squared_error(gdf_test[variable].values, y_pred_test)))\n",
    "\n",
    "    # add correlation of explanatories\n",
    "    for expl in temporal_expl:\n",
    "        explans.append(gdf_test[variable].corr(gdf_test[expl]))\n",
    "     \n",
    "    row[station] = explans\n",
    "    #row[station] = [np.sqrt(mean_squared_error(gdf_test[variable].values, y_pred_test))]\n",
    "    #print(row)\n",
    "    \n",
    "stats_df = pd.DataFrame.from_dict(row, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"r_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_heatmap(stats_df, \"rmse_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_worse_stations = stats_df[stats_df.rmse_local < 15].index\n",
    "best_worse_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf_high.to_file(\"data/0_shp/kalimantan_best_stations.gpkg\", driver=\"GPKG\")\n",
    "len(gdf_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final model bootstraping (test different combinations)\n",
    "\n",
    "The following cells will test different combinations of stations, provinces or phus.\n",
    "After each bootraping, combination, a result containing the average, min, max and median statistics of the different statistical parameters over all the iterations. \n",
    "\n",
    "This result will help to select what is the best combination of stations to produce the final data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gee_scripts.randomforest import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrap_stations = list(best_stations) # + list(best_worse_stations)\n",
    "len(bootstrap_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap with only best stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = training_df[training_df.id.isin(best_stations)]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=5, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap with best + best worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = training_df[training_df.id.isin(list(best_stations) + list(best_worse_stations))]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=5, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap by PHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.phu_id == 801) #this is the code for 'KHG Sungai Siak - Sungai Kampar'\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bootstrap with BRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.source.isin([\"brg\", \"old_brg\"])) & \n",
    "    (df.island == \"Kalimantan\")\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.source.isin([\"brg\", \"old_brg\"])) & \n",
    "    (df.island == \"Sumatera\")\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap with PKEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.source.isin([\"pkeg\"])) & \n",
    "    (df.island == \"Sumatera\")\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.source.isin([\"pkeg\"])) & \n",
    "    (df.island == \"Kalimantan\")\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap by regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.province.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_df = df[\n",
    "    (df.source.isin([\"brg\", \"brg_old\"])) & \n",
    "    (df.province == \"Central Kalimantan\")\n",
    "]\n",
    "bootstrap_result = bootstrap(df = selected_df, variable=\"gwl_cm\", iterations=10, train_size=0.8)\n",
    "bootstrap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final model selection\n",
    "\n",
    "After selecting the best combination of stations that present the best model statistics (r, rmse), the following cell can be used to train and store the last model, replace \"final_df\" with the filters that worked well in the bootraping models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filters of the best stations.\n",
    "df = pd.read_csv(\"data/9_clean_training_data/clean_training_data.csv\")\n",
    "final_df = df[(df.phu_id == 801)]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the stations with high correlation or ingest all the station of the PHU in the model\n",
    "high_corr_ids = pd.read_csv(\"data/high_corr_0.2_temporal_variables_station_ids.csv\")\n",
    "\n",
    "training_df = df[\n",
    "  #  (df.island == \"Kalimantan\") & don't use the selection by island, the selected stations come from the previous notebook\n",
    "    df.id.isin(high_corr_ids.id.unique())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gee_scripts.directories import model_path\n",
    "# Save model to file with not pickle (pickle is not safe) \n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gee_scripts.parameters import explain_vars, temporal_expl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the df for the model, either high correlation stations or all the stations in a selected PHU\n",
    "\n",
    "#final_df = df[(df.phu_id == 801)]#To selecT it based on the PHU\n",
    "\n",
    "#Select only the stations with high correlation or ingest all the station of the PHU in the model\n",
    "high_corr_ids = pd.read_csv(\"data/high_corr_0.3_temporal_variables_station_ids.csv\")\n",
    "high_corr_ids.columns = ['id']\n",
    "#final_df = df[df['id'].isin(high_corr_ids)]\n",
    "final_df = df[df.id.isin(high_corr_ids.id.unique())]#To selecT it based on correlations\n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = final_df.drop(columns=['gwl_cm'])  # Replace 'target_column' with the name of your target column\n",
    "y = final_df['gwl_cm']\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# X_train and y_train contain 70% of the data for training\n",
    "# X_test and y_test contain 30% of the data for testing\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'gwl_cm'\n",
    "\n",
    "# Define the filters of the best stations.\n",
    "final_df = df[\n",
    "    (df.phu_id == 801)\n",
    "]\n",
    "\n",
    "regr = get_regressor()\n",
    "regr.fit(X_train[explain_vars], y_train)\n",
    "\n",
    "# Define a name for this model, it will be overwritten if there's something before\n",
    "model_name = \"model_sungai_siak_sungai_kampar_phu_0.3_corr_0.3.joblib\"\n",
    "joblib.dump(regr, model_path/model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = regr.predict(X_test[explain_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have trained your RandomForestRegressor model and made predictions\n",
    "# rf_regressor.fit(X_train, y_train)  # Assuming you've trained the model already\n",
    "# predictions = rf_regressor.predict(X_test)  # Assuming you've made predictions already\n",
    "\n",
    "# Calculate R-squared (coefficient of determination)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Plot real vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, color='blue', label='Predicted')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red', label='Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values (R-squared: {:.2f})'.format(r2))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwl-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
