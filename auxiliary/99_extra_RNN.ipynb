{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gee_scripts.parameters import explain_vars, response_var, west_region_ids, center_region_ids\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "# importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/field_data_unique_coords_and_location.csv\", parse_dates=[\"date\"])\n",
    "df = df[(df.gwl_cm <= 400)&(df.date.dt.year>2018)]\n",
    "df.set_index(\"date\", inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "# Remove all that is above 400cm\n",
    "# df = df[[\"date\", \"gwl_cm\"]]\n",
    "len(df)\n",
    "\n",
    "kalimantan_df = df[df.region_id.isin(center_region_ids)][[\"gwl_cm\"]]\n",
    "sumatra_df = df[df.region_id.isin(west_region_ids)][[\"gwl_cm\"]]\n",
    "\n",
    "\n",
    "# data = df.sort_values(by=\"date\")\n",
    "# Set date as index\n",
    "\n",
    "# sort by date ascending\n",
    "\n",
    "# Aggregate data every 7 days using mean\n",
    "kalimantan_resampled = kalimantan_df.resample('3D').mean()\n",
    "sumatra_resampled = sumatra_df.resample('3D').mean()\n",
    "\n",
    "# Plot the data in two subplots using seaborn\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "kalimantan_resampled.plot(ax=ax[0], title=\"Kalimantan\")\n",
    "sumatra_resampled.plot(ax=ax[1], title=\"Sumatra\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimantan_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kalimantan_df[[\"gwl_cm\"]]\n",
    "\n",
    "# Setting 80 percent data for training\n",
    "training_data_len = math.ceil(len(data) * .8)\n",
    "training_data_len \n",
    "\n",
    "#Splitting the dataset\n",
    "train_data = data[:training_data_len]\n",
    "test_data = data[training_data_len:]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_data.gwl_cm.values \n",
    "# Reshaping 1D to 2D array\n",
    "dataset_train = np.reshape(dataset_train, (-1,1)) \n",
    "dataset_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaling dataset\n",
    "scaled_train = scaler.fit_transform(dataset_train)\n",
    "\n",
    "print(scaled_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = test_data.gwl_cm.values \n",
    "# Reshaping 1D to 2D array\n",
    "dataset_test = np.reshape(dataset_test, (-1,1)) \n",
    "# Normalizing values between 0 and 1\n",
    "scaled_test = scaler.fit_transform(dataset_test) \n",
    "print(*scaled_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(50, len(scaled_train)):\n",
    "\tX_train.append(scaled_train[i-50:i, 0])\n",
    "\ty_train.append(scaled_train[i, 0])\n",
    "\tif i <= 51:\n",
    "\t\tprint(X_train)\n",
    "\t\tprint(y_train)\n",
    "\t\tprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(50, len(scaled_test)):\n",
    "\tX_test.append(scaled_test[i-50:i, 0])\n",
    "\ty_test.append(scaled_test[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is converted to Numpy array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "#Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "print(\"X_train :\",X_train.shape,\"y_train :\",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is converted to numpy array\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "#Reshaping\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0],1))\n",
    "print(\"X_test :\",X_test.shape,\"y_test :\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name: str = 'model'):\n",
    "\n",
    "    checkpoint_name = model_name + 'epoch_{epoch:02d}.h5'\n",
    "\n",
    "    # Setup model checkpoint\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_name,  # Saves the model with the epoch number in the filename\n",
    "        save_weights_only=False,       # Can set to True if you only want to save weights\n",
    "        save_best_only=False,          # Every epoch's model will be saved, not just the best\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # Setup early stopping\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='loss',  # Monitor validation loss (or adjust according to your setup)\n",
    "        min_delta=0.001,  # Minimum change to qualify as an improvement\n",
    "        patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=1,\n",
    "        restore_best_weights=True  # This rolls back to the best model\n",
    "    )\n",
    "\n",
    "    # Create output logs directory\n",
    "    Path(f'data/15_nn_logs/{model_name}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # tensorboard monitor\n",
    "    tensor_board = TensorBoard(\n",
    "        log_dir=f'data/15_nn_logs/{model_name}',\n",
    "        write_graph=True, \n",
    "        write_images=True,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "\n",
    "    return [checkpoint_callback, early_stopping_callback, tensor_board]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# adding RNN layers and dropout regularization\n",
    "regressor.add(SimpleRNN(units = 50, \n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True,\n",
    "\t\t\t\t\t\tinput_shape = (X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 50, \n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 50,\n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 50))\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units = 1,activation='sigmoid'))\n",
    "\n",
    "# compiling RNN\n",
    "regressor.compile(optimizer = SGD(learning_rate=0.01,\n",
    "\t\t\t\t\t\t\t\tdecay=1e-6, \n",
    "\t\t\t\t\t\t\t\tmomentum=0.9, \n",
    "\t\t\t\t\t\t\t\tnesterov=True), \n",
    "\t\t\t\tloss = \"mean_squared_error\")\n",
    "\n",
    "# fitting the model\n",
    "regressor.fit(X_train, y_train, epochs = 1, batch_size = 32, callbacks=get_callbacks(\"simple_rnn\"))\n",
    "regressor.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the model\n",
    "regressorLSTM = Sequential()\n",
    "\n",
    "#Adding LSTM layers\n",
    "regressorLSTM.add(LSTM(50, return_sequences = True, input_shape = (X_train.shape[1],1)))\n",
    "regressorLSTM.add(LSTM(50, return_sequences = False))\n",
    "regressorLSTM.add(Dense(25))\n",
    "\n",
    "#Adding the output layer\n",
    "regressorLSTM.add(Dense(1))\n",
    "\n",
    "#Compiling the model\n",
    "regressorLSTM.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = [\"accuracy\"])\n",
    "\n",
    "#Fitting the model\n",
    "regressorLSTM.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size = 8, \n",
    "    epochs = 12, \n",
    "    callbacks=get_callbacks(\"LSTM\")\n",
    ")\n",
    "regressorLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the model\n",
    "regressorGRU = Sequential()\n",
    "\n",
    "# GRU layers with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, activation='tanh'))\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, activation='tanh'))\n",
    "regressorGRU.add(GRU(units=50, activation='tanh'))\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(learning_rate=0.01, decay=1e-7, momentum=0.9, nesterov=False), loss='mean_squared_error')\n",
    "\n",
    "# Fitting the data\n",
    "regressorGRU.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=1, \n",
    "    callbacks=get_callbacks(\"GRU\")\n",
    ")\n",
    "\n",
    "regressorGRU.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with X_test data\n",
    "# y_RNN = regressor.predict(X_test)\n",
    "y_LSTM = regressorLSTM.predict(X_test)\n",
    "y_GRU = regressorGRU.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling back from 0-1 to original\n",
    "# y_RNN_O = scaler.inverse_transform(y_RNN) \n",
    "y_LSTM_O = scaler.inverse_transform(y_LSTM) \n",
    "y_GRU_O = scaler.inverse_transform(y_GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,figsize =(18,12),sharex=True, sharey=True)\n",
    "fig.suptitle('Model Predictions')\n",
    "\n",
    "# #Plot for RNN predictions\n",
    "# axs[0].plot(train_data.index[150:], train_data.gwl_cm[150:], label = \"train_data\", color = \"b\")\n",
    "# axs[0].plot(test_data.index, test_data.gwl_cm, label = \"test_data\", color = \"g\")\n",
    "# axs[0].plot(test_data.index[50:], y_RNN_O, label = \"y_RNN\", color = \"brown\")\n",
    "# axs[0].legend()\n",
    "# axs[0].title.set_text(\"Basic RNN\")\n",
    "\n",
    "#Plot for LSTM predictions\n",
    "axs[1].plot(train_data.index[150:], train_data.gwl_cm[150:], label = \"train_data\", color = \"b\")\n",
    "axs[1].plot(test_data.index, test_data.gwl_cm, label = \"test_data\", color = \"g\")\n",
    "axs[1].plot(test_data.index[50:], y_LSTM_O, label = \"y_LSTM\", color = \"orange\")\n",
    "axs[1].legend()\n",
    "axs[1].title.set_text(\"LSTM\")\n",
    "\n",
    "#Plot for GRU predictions\n",
    "axs[2].plot(train_data.index[150:], train_data.gwl_cm[150:], label = \"train_data\", color = \"b\")\n",
    "axs[2].plot(test_data.index, test_data.gwl_cm, label = \"test_data\", color = \"g\")\n",
    "axs[2].plot(test_data.index[50:], y_GRU_O, label = \"y_GRU\", color = \"red\")\n",
    "axs[2].legend()\n",
    "axs[2].title.set_text(\"GRU\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"gwl_cm price\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph of observed vs predicted\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(25, 5))\n",
    "\n",
    "ax.plot(test_data.index[50:], test_data.gwl_cm[50:], label=\"Observed\", color=\"b\")\n",
    "ax.plot(test_data.index[50:], y_RNN_O, label=\"Predicted\", color=\"r\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(test) gwl-modeling",
   "language": "python",
   "name": "gwl-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
