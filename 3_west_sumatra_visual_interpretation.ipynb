{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from gee_scripts.parameters import explain_vars, response_var, west_region_ids, center_region_ids\n",
    "from scipy.stats import pearsonr\n",
    "from gee_scripts.models import get_random_forest, get_regressors\n",
    "import tensorflow as tf\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from gee_scripts.models import split_dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the training_df into training and test where test is the last year\n",
    "\n",
    "# # Train by year\n",
    "# # train_data = train_df[train_df[\"date\"].dt.year.isin([2020,2021,2022])]\n",
    "# # test_data = train_df[train_df[\"date\"].dt.year.isin([2023])]\n",
    "\n",
    "# # # train and test selection by month\n",
    "# # train_data = train_df[train_df[\"date\"].dt.month.isin([1,2,4,5,7,8,10,11,12])]\n",
    "# # test_data = train_df[train_df[\"date\"].dt.month.isin([3,6,9])]\n",
    "\n",
    "# X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "# y_train, y_test = train_data[\"gwl_cm\"], test_data[\"gwl_cm\"]\n",
    "\n",
    "# print(\"lenght of train and test\", len(X_train), len(X_test))\n",
    "\n",
    "# ####################### TRAIN THE MODEL ############################\n",
    "\n",
    "# regr.fit(X_train, y_train)\n",
    "# y_pred_test = regr.predict(X_test)\n",
    "\n",
    "# r, p = pearsonr(y_test, y_pred_test)\n",
    "# r2_score_val = r2_score(y_test, y_pred_test)\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# # print all the metrics\n",
    "# print(f\"r2_score: {r2_score_val}\")\n",
    "# print(f\"rmse: {rmse}\")\n",
    "# print(f\"pearson r: {r}\")\n",
    "# print(f\"p-value: {p}\")\n",
    "\n",
    "# # Plot the observed vs predicted with labels and title\n",
    "# plt.scatter(y_test, y_pred_test)\n",
    "# plt.xlabel(\"Observed\")\n",
    "# plt.ylabel(\"Predicted\")\n",
    "# plt.title(\"Observed vs Predicted\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5449"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/9_clean_training_data/all_training_data_with_extra_and_locations_and_precipSum.csv\", parse_dates=[\"date\"])\n",
    "df = df[df.region_id.isin(center_region_ids)]\n",
    "# assert df[[\"date\"]].dtypes.iloc[0] == \"datetime64[ns]\"\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter_condition = df.phu_id==71\n",
    "# train_df = df[filter_condition]\n",
    "# len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a boxplot of response var per region but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "# sns.set_style(\"whitegrid\");\n",
    "# sns.set(rc={'figure.figsize':(8,5)});\n",
    "# sns.boxplot(x=\"region_id\", y=\"gwl_cm\", data=train_df, width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a table with the count of the number of observations per month\n",
    "# train_df[\"month\"] = train_df[\"date\"].dt.month\n",
    "# train_df[\"year\"] = train_df[\"date\"].dt.year\n",
    "# train_df[\"month_year\"] = train_df[\"date\"].dt.to_period('M')\n",
    "# train_df[train_df.id == \"15_RAPP_LGBI-001a\"].groupby(\"month_year\").size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\n",
    "# get_precipitation_plot(train_df, group_by=\"station\", value=\"15_RAPP_LGBI-001a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group by id and get the number of dates for each id\n",
    "# group_by = \"id\"\n",
    "# df_grouped = train_df.groupby(group_by).count().reset_index()\n",
    "# df_grouped = df_grouped[[group_by, \"date\"]]\n",
    "# df_grouped.columns = [\"name\", \"date_count\"]\n",
    "# df_grouped.sort_values(by=\"date_count\", ascending=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all stations with less tan 9 observations\n",
    "# min_obs = 0\n",
    "# train_df.groupby('id').agg({'date': 'count'}).sort_values(by='date', ascending=False).reset_index()\n",
    "# train_df = train_df.groupby('id').filter(lambda group: len(group) >= min_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running cross validation on 23 phu_ids\n",
    "# processing phu_id 407.0 with No. of observations 199\n",
    "# Splitting by station with 34 categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df[df.phu_id==407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting by station with 199 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(iter(split_dataset(x_df, test_size=0.2, by=\"station\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting by year with 199 samples\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = next(iter(split_dataset(x_df, test_size=0.2, by=\"year\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(data, target_column, n_splits):\n",
    "    \"\"\"Evaluate the performance of different regression models on the dataset using cross-validation.\"\"\"\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Loop through each split type\n",
    "    for split_type in [\"station\", \"year\", \"month\"]:\n",
    "        \n",
    "        train_test_splits = split_dataset(data, by=split_type, n_splits=n_splits)\n",
    "\n",
    "        if split_type == \"station\":\n",
    "            split_type = \"id\"\n",
    "\n",
    "        for train_data, test_data in train_test_splits:\n",
    "\n",
    "            if len(test_data) == 0:\n",
    "                results.append({\n",
    "                    \"phu_id\": data[\"phu_id\"].iloc[0],\n",
    "                    \"split_type\": split_type,\n",
    "                    \"no_obs\": len(data),\n",
    "                    \"train_obs\": 0,\n",
    "                    \"test_obs\": 0,\n",
    "                    \"estimator_name\": None,\n",
    "                    \"validation\" : f\"cross validation {split_type}\",\n",
    "                    \"test_ids\": None,\n",
    "                    \"train_ids\": None,\n",
    "                    \"r2_score\": None,\n",
    "                    \"rmse\": None,\n",
    "                    \"pearson_r\": None,\n",
    "                    \"p_value\": None,\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "            y_train, y_test = train_data[target_column], test_data[target_column]\n",
    "\n",
    "            input_dim = X_train.shape[1]  # Number of explanatory variables\n",
    "\n",
    "            for regr in get_regressors(input_dim):\n",
    "                print(f\"Training {regr.__class__.__name__} on {len(train_data)} observations and testing on {len(test_data)} observations\")\n",
    "\n",
    "                if isinstance(regr, tf.keras.Model):  # Check if the model is a Keras model\n",
    "                    # Neural network requires normalization and batch processing\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_test_scaled = scaler.transform(X_test)\n",
    "                    \n",
    "                    # Fit the model\n",
    "                    regr.fit(X_train_scaled, y_train, epochs=5, batch_size=8, verbose=0)\n",
    "                    \n",
    "                    # Predict\n",
    "                    y_pred_test = regr.predict(X_test_scaled).flatten()  # Flatten to convert 2D predictions to 1D\n",
    "                else:\n",
    "                    # Fit traditional models\n",
    "                    regr.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Predict\n",
    "                    y_pred_test = regr.predict(X_test)\n",
    "\n",
    "\n",
    "                r, p = pearsonr(y_test, y_pred_test)\n",
    "                r2_score_val = r2_score(y_test, y_pred_test)\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "                results.append({\n",
    "                    \"phu_id\": train_data[\"phu_id\"].iloc[0],\n",
    "                    \"no_obs\": len(data),\n",
    "                    \"train_obs\": len(train_data),\n",
    "                    \"test_obs\": len(test_data),\n",
    "                    \"estimator_name\": regr.__class__.__name__,\n",
    "                    \"validation\" : f\"cross validation {split_type}\",\n",
    "                    \"train_ids\": train_data[split_type].unique(),\n",
    "                    \"test_ids\": test_data[split_type].unique(),\n",
    "                    \"r2_score\": r2_score_val,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"pearson_r\": r,\n",
    "                    \"p_value\": p,\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running cross validation on 23 phu_ids\n",
      "processing phu_id 407.0 with No. of observations 199\n",
      "Splitting by station with 199 samples\n",
      "Training RandomForestRegressor on 157 observations and testing on 42 observations\n",
      "Training GradientBoostingRegressor on 157 observations and testing on 42 observations\n",
      "Training LinearRegression on 157 observations and testing on 42 observations\n",
      "Training Sequential on 157 observations and testing on 42 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 148 observations and testing on 51 observations\n",
      "Training GradientBoostingRegressor on 148 observations and testing on 51 observations\n",
      "Training LinearRegression on 148 observations and testing on 51 observations\n",
      "Training Sequential on 148 observations and testing on 51 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 165 observations and testing on 34 observations\n",
      "Training GradientBoostingRegressor on 165 observations and testing on 34 observations\n",
      "Training LinearRegression on 165 observations and testing on 34 observations\n",
      "Training Sequential on 165 observations and testing on 34 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 163 observations and testing on 36 observations\n",
      "Training GradientBoostingRegressor on 163 observations and testing on 36 observations\n",
      "Training LinearRegression on 163 observations and testing on 36 observations\n",
      "Training Sequential on 163 observations and testing on 36 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 160 observations and testing on 39 observations\n",
      "Training GradientBoostingRegressor on 160 observations and testing on 39 observations\n",
      "Training LinearRegression on 160 observations and testing on 39 observations\n",
      "Training Sequential on 160 observations and testing on 39 observations\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x71fa9b4baef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 152 observations and testing on 47 observations\n",
      "Training GradientBoostingRegressor on 152 observations and testing on 47 observations\n",
      "Training LinearRegression on 152 observations and testing on 47 observations\n",
      "Training Sequential on 152 observations and testing on 47 observations\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x71fa93f972e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Training RandomForestRegressor on 161 observations and testing on 38 observations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m filter_condition \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mphu_id\u001b[38;5;241m==\u001b[39mphu_id\n\u001b[1;32m      8\u001b[0m train_df \u001b[38;5;241m=\u001b[39m df[filter_condition]\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgwl_cm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m phu_cross_validation_results\u001b[38;5;241m.\u001b[39mextend(results)\n",
      "Cell \u001b[0;32mIn[103], line 58\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(data, target_column, n_splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m     y_pred_test \u001b[38;5;241m=\u001b[39m regr\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Flatten to convert 2D predictions to 1D\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Fit traditional models\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     y_pred_test \u001b[38;5;241m=\u001b[39m regr\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phu_cross_validation_results = []\n",
    "print(f\"running cross validation on {df.phu_id.nunique()} phu_ids\")\n",
    "for phu_id in df.phu_id.unique():\n",
    "    if  pd.isnull(phu_id):\n",
    "        continue\n",
    "    print(\"processing phu_id\", phu_id, \"with No. of observations\", len(df[df.phu_id==phu_id]))\n",
    "    filter_condition = df.phu_id==phu_id\n",
    "    train_df = df[filter_condition]\n",
    "    results = evaluate_models(train_df, \"gwl_cm\", n_splits=10)\n",
    "    phu_cross_validation_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the results\n",
    "results_df = pd.DataFrame(phu_cross_validation_results)\n",
    "# results_df.to_csv(\"data/13_estimation_results/cross_validation_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by r2_score and phu_id\n",
    "results_df.sort_values(by=[\"r2_score\", \"phu_id\"], ascending=False, inplace=True)\n",
    "results_df.to_csv(\"data/13_estimation_results/center_multiple_models_cross_validation_results_sorted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(evaluation_results)\n",
    "plt.title('Model Performance Across Random Splits')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average RMSE across splits:\", np.mean(evaluation_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA TEST\n",
    "\n",
    "# Divide train and test by PCA and year\n",
    "# train_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & train_df[\"date\"].dt.year.isin([2020,2021,2022])]\n",
    "# test_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & train_df[\"date\"].dt.year.isin([2023])]\n",
    "\n",
    "# # Divide train and test by PCA and month\n",
    "train_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & (train_df[\"date\"].dt.month.isin([1,2,4,5,7,8,10,11,12]))]\n",
    "test_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & (train_df[\"date\"].dt.month.isin([3,6,9]))]\n",
    "\n",
    "\n",
    "X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "y_train, y_test = train_data[\"gwl_cm\"], test_data[\"gwl_cm\"]\n",
    "\n",
    "print(\"lenght of train and test\", len(X_train), len(X_test))\n",
    "\n",
    "####################### TRAIN\n",
    "\n",
    "regr = get_random_forest()\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred_test = regr.predict(X_test)\n",
    "\n",
    "r, p = pearsonr(y_test, y_pred_test)\n",
    "r2_score_val = r2_score(y_test, y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# print all the metrics\n",
    "print(f\"r2_score: {r2_score_val}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"pearson r: {r}\")\n",
    "print(f\"p-value: {p}\")\n",
    "\n",
    "# Plot the observed vs predicted with labels and title\n",
    "plt.scatter(y_test, y_pred_test)\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Observed vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"dependent var\", response_var)\n",
    "print(\"explanatory lenght\", len(explain_vars))\n",
    "print(explain_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap_result = bootstrap(df = train_df, variable=\"gwl_cm\", iterations=100, train_size=0.8, explain_vars=explain_vars, bootstrap_by=\"observations\")\n",
    "# bootstrap_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to test the model on unseen data and see how it performs\n",
    "from gee_scripts.models import get_random_forest\n",
    "\n",
    "# train the model on all the data\n",
    "regressor = get_random_forest()\n",
    "regressor.fit(train_df[explain_vars], train_df['gwl_cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 350, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Assume get_regressor() is your function to get the base model\n",
    "base_model = get_regressor()\n",
    "grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "X_train, y_train = df[explain_vars], df['gwl_cm']\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on unseen data\n",
    "test_df = df[df.phu_id == 169]\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test, y_test = test_df[explain_vars], test_df['gwl_cm']\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "r, p = pearsonr(y_test, y_pred)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}).plot.scatter(x=\"y_test\", y=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwl-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
