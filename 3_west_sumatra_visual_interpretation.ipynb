{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 09:48:54.437986: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 09:48:54.477586: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 09:48:54.698949: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 09:48:54.701170: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 09:48:55.514692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from gee_scripts.parameters import explain_vars, response_var, west_region_ids, center_region_ids\n",
    "from scipy.stats import pearsonr\n",
    "from gee_scripts.models import get_random_forest, get_regressors\n",
    "import tensorflow as tf\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the training_df into training and test where test is the last year\n",
    "\n",
    "# # Train by year\n",
    "# # train_data = train_df[train_df[\"date\"].dt.year.isin([2020,2021,2022])]\n",
    "# # test_data = train_df[train_df[\"date\"].dt.year.isin([2023])]\n",
    "\n",
    "# # # train and test selection by month\n",
    "# # train_data = train_df[train_df[\"date\"].dt.month.isin([1,2,4,5,7,8,10,11,12])]\n",
    "# # test_data = train_df[train_df[\"date\"].dt.month.isin([3,6,9])]\n",
    "\n",
    "# X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "# y_train, y_test = train_data[\"gwl_cm\"], test_data[\"gwl_cm\"]\n",
    "\n",
    "# print(\"lenght of train and test\", len(X_train), len(X_test))\n",
    "\n",
    "# ####################### TRAIN THE MODEL ############################\n",
    "\n",
    "# regr.fit(X_train, y_train)\n",
    "# y_pred_test = regr.predict(X_test)\n",
    "\n",
    "# r, p = pearsonr(y_test, y_pred_test)\n",
    "# r2_score_val = r2_score(y_test, y_pred_test)\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# # print all the metrics\n",
    "# print(f\"r2_score: {r2_score_val}\")\n",
    "# print(f\"rmse: {rmse}\")\n",
    "# print(f\"pearson r: {r}\")\n",
    "# print(f\"p-value: {p}\")\n",
    "\n",
    "# # Plot the observed vs predicted with labels and title\n",
    "# plt.scatter(y_test, y_pred_test)\n",
    "# plt.xlabel(\"Observed\")\n",
    "# plt.ylabel(\"Predicted\")\n",
    "# plt.title(\"Observed vs Predicted\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5449"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/9_clean_training_data/all_training_data_with_extra_and_locations_and_precipSum.csv\", parse_dates=[\"date\"])\n",
    "df = df[df.region_id.isin(center_region_ids)]\n",
    "# assert df[[\"date\"]].dtypes.iloc[0] == \"datetime64[ns]\"\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2702"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_condition = df.phu_id==71\n",
    "train_df = df[filter_condition]\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a boxplot of response var per region but use a small graph size\n",
    "\n",
    "# set the seaborn style and size\n",
    "# sns.set_style(\"whitegrid\");\n",
    "# sns.set(rc={'figure.figsize':(8,5)});\n",
    "# sns.boxplot(x=\"region_id\", y=\"gwl_cm\", data=train_df, width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a table with the count of the number of observations per month\n",
    "# train_df[\"month\"] = train_df[\"date\"].dt.month\n",
    "# train_df[\"year\"] = train_df[\"date\"].dt.year\n",
    "# train_df[\"month_year\"] = train_df[\"date\"].dt.to_period('M')\n",
    "# train_df[train_df.id == \"15_RAPP_LGBI-001a\"].groupby(\"month_year\").size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\n",
    "# get_precipitation_plot(train_df, group_by=\"station\", value=\"15_RAPP_LGBI-001a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group by id and get the number of dates for each id\n",
    "# group_by = \"id\"\n",
    "# df_grouped = train_df.groupby(group_by).count().reset_index()\n",
    "# df_grouped = df_grouped[[group_by, \"date\"]]\n",
    "# df_grouped.columns = [\"name\", \"date_count\"]\n",
    "# df_grouped.sort_values(by=\"date_count\", ascending=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop all stations with less tan 9 observations\n",
    "# min_obs = 0\n",
    "# train_df.groupby('id').agg({'date': 'count'}).sort_values(by='date', ascending=False).reset_index()\n",
    "# train_df = train_df.groupby('id').filter(lambda group: len(group) >= min_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, Literal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(df: pd.DataFrame, by: Literal[\"year\", \"month\", \"station\"], n_splits=5, min_samples_per_category=10) -> Tuple[list, list]:\n",
    "    \"\"\"Split the dataset into training and test sets by specified 'by' category with approximately 80/20 distribution,\n",
    "    taking into account categories with limited data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the data to be split.\n",
    "        by (Literal): The category by which to split ('year', 'month', or 'station').\n",
    "        n_splits (int): The number of splits (default 5).\n",
    "        min_samples_per_category (int): Minimum samples per category to proceed with a split.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[list, list]: A tuple containing lists of dataframes representing the training and test splits.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Assert that the date column is in datetime format in all cases\n",
    "    assert df[\"date\"].dtypes == \"datetime64[ns]\"\n",
    "    \n",
    "    if by == \"year\":\n",
    "        df['year'] = df['date'].dt.year\n",
    "        split_column = 'year'\n",
    "\n",
    "    elif by == \"month\":\n",
    "        df['month'] = df['date'].dt.month\n",
    "        split_column = 'month'\n",
    "\n",
    "    if by == \"station\":\n",
    "        split_column = \"id\"\n",
    "\n",
    "    train_splits = []\n",
    "    test_splits = []\n",
    "    categories = df[split_column].unique()\n",
    "\n",
    "    for _ in range(n_splits):\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "\n",
    "        for category in categories:\n",
    "            subset = df[df[split_column] == category]\n",
    "            # Check if there are enough samples to split\n",
    "            if len(subset) < min_samples_per_category:\n",
    "                continue  # or handle this scenario differently as needed\n",
    "            test_size = 0.2 if len(subset) * 0.2 > 5 else len(subset) * 0.1  # Adjust test size if too few samples\n",
    "            train, test = train_test_split(subset, test_size=test_size, random_state=None)\n",
    "            train_list.append(train)\n",
    "            test_list.append(test)\n",
    "        \n",
    "        train_splits.append(pd.concat(train_list, ignore_index=True))\n",
    "        test_splits.append(pd.concat(test_list, ignore_index=True))\n",
    "        \n",
    "    return train_splits, test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models(data, target_column, n_splits):\n",
    "    \"\"\"Evaluate the performance of different regression models on the dataset using cross-validation.\"\"\"\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Loop through each split type\n",
    "    for split_type in [\"station\", \"year\", \"month\"]:\n",
    "        \n",
    "        train_list, test_list = split_dataset(data, by=split_type, n_splits=n_splits)\n",
    "\n",
    "        for train_data, test_data in zip(train_list, test_list):\n",
    "\n",
    "\n",
    "            if len(test_data) == 0:\n",
    "                results.append({\n",
    "                    \"phu_id\": data[\"phu_id\"].iloc[0],\n",
    "                    \"split_type\": split_type,\n",
    "                    \"no_obs\": len(data),\n",
    "                    \"train_obs\": 0,\n",
    "                    \"test_obs\": 0,\n",
    "                    \"estimator_name\": None,\n",
    "                    \"validation\" : f\"cross validation {split_type}\",\n",
    "                    \"test_ids\": None,\n",
    "                    \"train_ids\": None,\n",
    "                    \"r2_score\": None,\n",
    "                    \"rmse\": None,\n",
    "                    \"pearson_r\": None,\n",
    "                    \"p_value\": None,\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "            y_train, y_test = train_data[target_column], test_data[target_column]\n",
    "\n",
    "            input_dim = X_train.shape[1]  # Number of explanatory variables\n",
    "\n",
    "            for regr in get_regressors(input_dim):\n",
    "                print(f\"Training {regr.__class__.__name__} on {len(train_data)} observations and testing on {len(test_data)} observations\")\n",
    "\n",
    "                if isinstance(regr, tf.keras.Model):  # Check if the model is a Keras model\n",
    "                    # Neural network requires normalization and batch processing\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_test_scaled = scaler.transform(X_test)\n",
    "                    \n",
    "                    # Fit the model\n",
    "                    regr.fit(X_train_scaled, y_train, epochs=5, batch_size=8, verbose=0)\n",
    "                    \n",
    "                    # Predict\n",
    "                    y_pred_test = regr.predict(X_test_scaled).flatten()  # Flatten to convert 2D predictions to 1D\n",
    "                else:\n",
    "                    # Fit traditional models\n",
    "                    regr.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Predict\n",
    "                    y_pred_test = regr.predict(X_test)\n",
    "\n",
    "\n",
    "                r, p = pearsonr(y_test, y_pred_test)\n",
    "                r2_score_val = r2_score(y_test, y_pred_test)\n",
    "                rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "                results.append({\n",
    "                    \"phu_id\": train_data[\"phu_id\"].iloc[0],\n",
    "                    \"no_obs\": len(data),\n",
    "                    \"train_obs\": len(train_data),\n",
    "                    \"test_obs\": len(test_data),\n",
    "                    \"estimator_name\": regr.__class__.__name__,\n",
    "                    \"validation\" : f\"cross validation {split_type}\",\n",
    "                    \"train_ids\": train_data[train_data[split_type]].unique(),\n",
    "                    \"test_ids\": test_data[test_data[split_type]].unique(),\n",
    "                    \"r2_score\": r2_score_val,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"pearson_r\": r,\n",
    "                    \"p_value\": p,\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199   NaN\n",
       "200   NaN\n",
       "201   NaN\n",
       "202   NaN\n",
       "Name: phu_id, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.phu_id[df.phu_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running cross validation on 23 phu_ids\n",
      "processing phu_id 407.0\n",
      "Training RandomForestRegressor on 166 observations and testing on 33 observations\n",
      "Training GradientBoostingRegressor on 166 observations and testing on 33 observations\n",
      "Training LinearRegression on 166 observations and testing on 33 observations\n",
      "Training Sequential on 166 observations and testing on 33 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Training RandomForestRegressor on 161 observations and testing on 38 observations\n",
      "Training GradientBoostingRegressor on 161 observations and testing on 38 observations\n",
      "Training LinearRegression on 161 observations and testing on 38 observations\n",
      "Training Sequential on 161 observations and testing on 38 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 158 observations and testing on 41 observations\n",
      "Training GradientBoostingRegressor on 158 observations and testing on 41 observations\n",
      "Training LinearRegression on 158 observations and testing on 41 observations\n",
      "Training Sequential on 158 observations and testing on 41 observations\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Training RandomForestRegressor on 157 observations and testing on 42 observations\n",
      "Training GradientBoostingRegressor on 157 observations and testing on 42 observations\n",
      "Training LinearRegression on 157 observations and testing on 42 observations\n",
      "Training Sequential on 157 observations and testing on 42 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Training RandomForestRegressor on 164 observations and testing on 35 observations\n",
      "Training GradientBoostingRegressor on 164 observations and testing on 35 observations\n",
      "Training LinearRegression on 164 observations and testing on 35 observations\n",
      "Training Sequential on 164 observations and testing on 35 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "processing phu_id 404.0\n",
      "Training RandomForestRegressor on 191 observations and testing on 33 observations\n",
      "Training GradientBoostingRegressor on 191 observations and testing on 33 observations\n",
      "Training LinearRegression on 191 observations and testing on 33 observations\n",
      "Training Sequential on 191 observations and testing on 33 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 179 observations and testing on 45 observations\n",
      "Training GradientBoostingRegressor on 179 observations and testing on 45 observations\n",
      "Training LinearRegression on 179 observations and testing on 45 observations\n",
      "Training Sequential on 179 observations and testing on 45 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 181 observations and testing on 43 observations\n",
      "Training GradientBoostingRegressor on 181 observations and testing on 43 observations\n",
      "Training LinearRegression on 181 observations and testing on 43 observations\n",
      "Training Sequential on 181 observations and testing on 43 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 172 observations and testing on 52 observations\n",
      "Training GradientBoostingRegressor on 172 observations and testing on 52 observations\n",
      "Training LinearRegression on 172 observations and testing on 52 observations\n",
      "Training Sequential on 172 observations and testing on 52 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Training RandomForestRegressor on 180 observations and testing on 44 observations\n",
      "Training GradientBoostingRegressor on 180 observations and testing on 44 observations\n",
      "Training LinearRegression on 180 observations and testing on 44 observations\n",
      "Training Sequential on 180 observations and testing on 44 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "processing phu_id 375.0\n",
      "Training RandomForestRegressor on 157 observations and testing on 25 observations\n",
      "Training GradientBoostingRegressor on 157 observations and testing on 25 observations\n",
      "Training LinearRegression on 157 observations and testing on 25 observations\n",
      "Training Sequential on 157 observations and testing on 25 observations\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Training RandomForestRegressor on 152 observations and testing on 30 observations\n",
      "Training GradientBoostingRegressor on 152 observations and testing on 30 observations\n",
      "Training LinearRegression on 152 observations and testing on 30 observations\n",
      "Training Sequential on 152 observations and testing on 30 observations\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Training RandomForestRegressor on 156 observations and testing on 26 observations\n",
      "Training GradientBoostingRegressor on 156 observations and testing on 26 observations\n",
      "Training LinearRegression on 156 observations and testing on 26 observations\n",
      "Training Sequential on 156 observations and testing on 26 observations\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Training RandomForestRegressor on 143 observations and testing on 39 observations\n",
      "Training GradientBoostingRegressor on 143 observations and testing on 39 observations\n",
      "Training LinearRegression on 143 observations and testing on 39 observations\n",
      "Training Sequential on 143 observations and testing on 39 observations\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 164 observations and testing on 18 observations\n",
      "Training GradientBoostingRegressor on 164 observations and testing on 18 observations\n",
      "Training LinearRegression on 164 observations and testing on 18 observations\n",
      "Training Sequential on 164 observations and testing on 18 observations\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "processing phu_id 256.0\n",
      "Training RandomForestRegressor on 671 observations and testing on 178 observations\n",
      "Training GradientBoostingRegressor on 671 observations and testing on 178 observations\n",
      "Training LinearRegression on 671 observations and testing on 178 observations\n",
      "Training Sequential on 671 observations and testing on 178 observations\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 680 observations and testing on 169 observations\n",
      "Training GradientBoostingRegressor on 680 observations and testing on 169 observations\n",
      "Training LinearRegression on 680 observations and testing on 169 observations\n",
      "Training Sequential on 680 observations and testing on 169 observations\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Training RandomForestRegressor on 749 observations and testing on 100 observations\n",
      "Training GradientBoostingRegressor on 749 observations and testing on 100 observations\n",
      "Training LinearRegression on 749 observations and testing on 100 observations\n",
      "Training Sequential on 749 observations and testing on 100 observations\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 701 observations and testing on 148 observations\n",
      "Training GradientBoostingRegressor on 701 observations and testing on 148 observations\n",
      "Training LinearRegression on 701 observations and testing on 148 observations\n",
      "Training Sequential on 701 observations and testing on 148 observations\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 687 observations and testing on 162 observations\n",
      "Training GradientBoostingRegressor on 687 observations and testing on 162 observations\n",
      "Training LinearRegression on 687 observations and testing on 162 observations\n",
      "Training Sequential on 687 observations and testing on 162 observations\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "processing phu_id 340.0\n",
      "Training RandomForestRegressor on 1194 observations and testing on 352 observations\n",
      "Training GradientBoostingRegressor on 1194 observations and testing on 352 observations\n",
      "Training LinearRegression on 1194 observations and testing on 352 observations\n",
      "Training Sequential on 1194 observations and testing on 352 observations\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "Training RandomForestRegressor on 1121 observations and testing on 425 observations\n",
      "Training GradientBoostingRegressor on 1121 observations and testing on 425 observations\n",
      "Training LinearRegression on 1121 observations and testing on 425 observations\n",
      "Training Sequential on 1121 observations and testing on 425 observations\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 1192 observations and testing on 354 observations\n",
      "Training GradientBoostingRegressor on 1192 observations and testing on 354 observations\n",
      "Training LinearRegression on 1192 observations and testing on 354 observations\n",
      "Training Sequential on 1192 observations and testing on 354 observations\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 1086 observations and testing on 460 observations\n",
      "Training GradientBoostingRegressor on 1086 observations and testing on 460 observations\n",
      "Training LinearRegression on 1086 observations and testing on 460 observations\n",
      "Training Sequential on 1086 observations and testing on 460 observations\n",
      "15/15 [==============================] - 0s 1ms/step\n",
      "Training RandomForestRegressor on 1238 observations and testing on 308 observations\n",
      "Training GradientBoostingRegressor on 1238 observations and testing on 308 observations\n",
      "Training LinearRegression on 1238 observations and testing on 308 observations\n",
      "Training Sequential on 1238 observations and testing on 308 observations\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "processing phu_id 352.0\n",
      "Training RandomForestRegressor on 218 observations and testing on 10 observations\n",
      "Training GradientBoostingRegressor on 218 observations and testing on 10 observations\n",
      "Training LinearRegression on 218 observations and testing on 10 observations\n",
      "Training Sequential on 218 observations and testing on 10 observations\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Training RandomForestRegressor on 141 observations and testing on 87 observations\n",
      "Training GradientBoostingRegressor on 141 observations and testing on 87 observations\n",
      "Training LinearRegression on 141 observations and testing on 87 observations\n",
      "Training Sequential on 141 observations and testing on 87 observations\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 219 observations and testing on 9 observations\n",
      "Training GradientBoostingRegressor on 219 observations and testing on 9 observations\n",
      "Training LinearRegression on 219 observations and testing on 9 observations\n",
      "Training Sequential on 219 observations and testing on 9 observations\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Training RandomForestRegressor on 215 observations and testing on 13 observations\n",
      "Training GradientBoostingRegressor on 215 observations and testing on 13 observations\n",
      "Training LinearRegression on 215 observations and testing on 13 observations\n",
      "Training Sequential on 215 observations and testing on 13 observations\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Training RandomForestRegressor on 200 observations and testing on 28 observations\n",
      "Training GradientBoostingRegressor on 200 observations and testing on 28 observations\n",
      "Training LinearRegression on 200 observations and testing on 28 observations\n",
      "Training Sequential on 200 observations and testing on 28 observations\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "processing phu_id 371.0\n",
      "Training RandomForestRegressor on 212 observations and testing on 68 observations\n",
      "Training GradientBoostingRegressor on 212 observations and testing on 68 observations\n",
      "Training LinearRegression on 212 observations and testing on 68 observations\n",
      "Training Sequential on 212 observations and testing on 68 observations\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 248 observations and testing on 32 observations\n",
      "Training GradientBoostingRegressor on 248 observations and testing on 32 observations\n",
      "Training LinearRegression on 248 observations and testing on 32 observations\n",
      "Training Sequential on 248 observations and testing on 32 observations\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Training RandomForestRegressor on 248 observations and testing on 32 observations\n",
      "Training GradientBoostingRegressor on 248 observations and testing on 32 observations\n",
      "Training LinearRegression on 248 observations and testing on 32 observations\n",
      "Training Sequential on 248 observations and testing on 32 observations\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Training RandomForestRegressor on 184 observations and testing on 96 observations\n",
      "Training GradientBoostingRegressor on 184 observations and testing on 96 observations\n",
      "Training LinearRegression on 184 observations and testing on 96 observations\n",
      "Training Sequential on 184 observations and testing on 96 observations\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Training RandomForestRegressor on 244 observations and testing on 36 observations\n",
      "Training GradientBoostingRegressor on 244 observations and testing on 36 observations\n",
      "Training LinearRegression on 244 observations and testing on 36 observations\n",
      "Training Sequential on 244 observations and testing on 36 observations\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "processing phu_id 314.0\n",
      "Training RandomForestRegressor on 24 observations and testing on 2 observations\n",
      "Training GradientBoostingRegressor on 24 observations and testing on 2 observations\n",
      "Training LinearRegression on 24 observations and testing on 2 observations\n",
      "Training Sequential on 24 observations and testing on 2 observations\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Training RandomForestRegressor on 22 observations and testing on 4 observations\n",
      "Training GradientBoostingRegressor on 22 observations and testing on 4 observations\n",
      "Training LinearRegression on 22 observations and testing on 4 observations\n",
      "Training Sequential on 22 observations and testing on 4 observations\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Training RandomForestRegressor on 22 observations and testing on 4 observations\n",
      "Training GradientBoostingRegressor on 22 observations and testing on 4 observations\n",
      "Training LinearRegression on 22 observations and testing on 4 observations\n",
      "Training Sequential on 22 observations and testing on 4 observations\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Training RandomForestRegressor on 22 observations and testing on 4 observations\n",
      "Training GradientBoostingRegressor on 22 observations and testing on 4 observations\n",
      "Training LinearRegression on 22 observations and testing on 4 observations\n",
      "Training Sequential on 22 observations and testing on 4 observations\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Training RandomForestRegressor on 20 observations and testing on 6 observations\n",
      "Training GradientBoostingRegressor on 20 observations and testing on 6 observations\n",
      "Training LinearRegression on 20 observations and testing on 6 observations\n",
      "Training Sequential on 20 observations and testing on 6 observations\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "processing phu_id 297.0\n",
      "Training RandomForestRegressor on 127 observations and testing on 28 observations\n",
      "Training GradientBoostingRegressor on 127 observations and testing on 28 observations\n",
      "Training LinearRegression on 127 observations and testing on 28 observations\n",
      "Training Sequential on 127 observations and testing on 28 observations\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Training RandomForestRegressor on 128 observations and testing on 27 observations\n",
      "Training GradientBoostingRegressor on 128 observations and testing on 27 observations\n",
      "Training LinearRegression on 128 observations and testing on 27 observations\n",
      "Training Sequential on 128 observations and testing on 27 observations\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Training RandomForestRegressor on 128 observations and testing on 27 observations\n",
      "Training GradientBoostingRegressor on 128 observations and testing on 27 observations\n",
      "Training LinearRegression on 128 observations and testing on 27 observations\n",
      "Training Sequential on 128 observations and testing on 27 observations\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Training RandomForestRegressor on 129 observations and testing on 26 observations\n",
      "Training GradientBoostingRegressor on 129 observations and testing on 26 observations\n",
      "Training LinearRegression on 129 observations and testing on 26 observations\n",
      "Training Sequential on 129 observations and testing on 26 observations\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Training RandomForestRegressor on 129 observations and testing on 26 observations\n",
      "Training GradientBoostingRegressor on 129 observations and testing on 26 observations\n",
      "Training LinearRegression on 129 observations and testing on 26 observations\n",
      "Training Sequential on 129 observations and testing on 26 observations\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "processing phu_id 253.0\n",
      "Training RandomForestRegressor on 26 observations and testing on 6 observations\n",
      "Training GradientBoostingRegressor on 26 observations and testing on 6 observations\n",
      "Training LinearRegression on 26 observations and testing on 6 observations\n",
      "Training Sequential on 26 observations and testing on 6 observations\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Training RandomForestRegressor on 26 observations and testing on 6 observations\n",
      "Training GradientBoostingRegressor on 26 observations and testing on 6 observations\n",
      "Training LinearRegression on 26 observations and testing on 6 observations\n",
      "Training Sequential on 26 observations and testing on 6 observations\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Training RandomForestRegressor on 30 observations and testing on 2 observations\n",
      "Training GradientBoostingRegressor on 30 observations and testing on 2 observations\n",
      "Training LinearRegression on 30 observations and testing on 2 observations\n",
      "Training Sequential on 30 observations and testing on 2 observations\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Training RandomForestRegressor on 27 observations and testing on 5 observations\n",
      "Training GradientBoostingRegressor on 27 observations and testing on 5 observations\n",
      "Training LinearRegression on 27 observations and testing on 5 observations\n",
      "Training Sequential on 27 observations and testing on 5 observations\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Training RandomForestRegressor on 27 observations and testing on 5 observations\n",
      "Training GradientBoostingRegressor on 27 observations and testing on 5 observations\n",
      "Training LinearRegression on 27 observations and testing on 5 observations\n",
      "Training Sequential on 27 observations and testing on 5 observations\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "processing phu_id 229.0\n",
      "Training RandomForestRegressor on 19 observations and testing on 1 observations\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have length at least 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1253826/2853060059.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing phu_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfilter_condition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphu_id\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mphu_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_condition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_month_split_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gwl_cm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mphu_cross_validation_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1253826/2647076144.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, target_column, n_splits)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mr2_score_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/module-venv/gwl-modeling/lib/python3.10/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, alternative, method)\u001b[0m\n\u001b[1;32m   4720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4721\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x and y must have the same length.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4724\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x and y must have length at least 2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4726\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4727\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have length at least 2."
     ]
    }
   ],
   "source": [
    "phu_cross_validation_results = []\n",
    "print(f\"running cross validation on {df.phu_id.nunique()} phu_ids\")\n",
    "for phu_id in df.phu_id.unique():\n",
    "    if  pd.isnull(phu_id):\n",
    "        continue\n",
    "    print(\"processing phu_id\", phu_id)\n",
    "    filter_condition = df.phu_id==phu_id\n",
    "    train_df = df[filter_condition]\n",
    "    results = random_month_split_evaluation(train_df, \"gwl_cm\", n_splits=5)\n",
    "    phu_cross_validation_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the results\n",
    "results_df = pd.DataFrame(phu_cross_validation_results)\n",
    "# results_df.to_csv(\"data/13_estimation_results/cross_validation_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by r2_score and phu_id\n",
    "results_df.sort_values(by=[\"r2_score\", \"phu_id\"], ascending=False, inplace=True)\n",
    "results_df.to_csv(\"data/13_estimation_results/center_multiple_models_cross_validation_results_sorted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(evaluation_results)\n",
    "plt.title('Model Performance Across Random Splits')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average RMSE across splits:\", np.mean(evaluation_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA TEST\n",
    "\n",
    "# Divide train and test by PCA and year\n",
    "# train_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & train_df[\"date\"].dt.year.isin([2020,2021,2022])]\n",
    "# test_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & train_df[\"date\"].dt.year.isin([2023])]\n",
    "\n",
    "# # Divide train and test by PCA and month\n",
    "train_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & (train_df[\"date\"].dt.month.isin([1,2,4,5,7,8,10,11,12]))]\n",
    "test_data = train_df[train_df.id.isin(pca1_and_pca2_1stq_ids) & (train_df[\"date\"].dt.month.isin([3,6,9]))]\n",
    "\n",
    "\n",
    "X_train, X_test = train_data[explain_vars], test_data[explain_vars]\n",
    "y_train, y_test = train_data[\"gwl_cm\"], test_data[\"gwl_cm\"]\n",
    "\n",
    "print(\"lenght of train and test\", len(X_train), len(X_test))\n",
    "\n",
    "####################### TRAIN\n",
    "\n",
    "regr = get_random_forest()\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred_test = regr.predict(X_test)\n",
    "\n",
    "r, p = pearsonr(y_test, y_pred_test)\n",
    "r2_score_val = r2_score(y_test, y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# print all the metrics\n",
    "print(f\"r2_score: {r2_score_val}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"pearson r: {r}\")\n",
    "print(f\"p-value: {p}\")\n",
    "\n",
    "# Plot the observed vs predicted with labels and title\n",
    "plt.scatter(y_test, y_pred_test)\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Observed vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"dependent var\", response_var)\n",
    "print(\"explanatory lenght\", len(explain_vars))\n",
    "print(explain_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap_result = bootstrap(df = train_df, variable=\"gwl_cm\", iterations=100, train_size=0.8, explain_vars=explain_vars, bootstrap_by=\"observations\")\n",
    "# bootstrap_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to test the model on unseen data and see how it performs\n",
    "from gee_scripts.models import get_random_forest\n",
    "\n",
    "# train the model on all the data\n",
    "regressor = get_random_forest()\n",
    "regressor.fit(train_df[explain_vars], train_df['gwl_cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 350, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Assume get_regressor() is your function to get the base model\n",
    "base_model = get_regressor()\n",
    "grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "X_train, y_train = df[explain_vars], df['gwl_cm']\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on unseen data\n",
    "test_df = df[df.phu_id == 169]\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test, y_test = test_df[explain_vars], test_df['gwl_cm']\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "r, p = pearsonr(y_test, y_pred)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}).plot.scatter(x=\"y_test\", y=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwl-modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
